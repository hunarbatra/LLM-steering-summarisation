{"nodes": [{"id": "-4295577623074310743", "text": "You are an intelligent summariser and a professional writer. You are given a piece of text and it's overview structure. You will use the text and overview structure to write a cohesive and detailed summary in 250 words step by step. You will cover all important aspects and key ideas presented in the text. Make sure that the generated text is not redundant, and it should be professionally summarised. Be sure to prioritise adding important details and information from the text in the summary.\n\n        Inducing human-like biases in moral reasoning LMs \n\nThis project is about fine-tuning language models (LMs) on a publicly available moral reasoning neuroimaging (fMRI) dataset, with the hope/expectation that this could help induce more human-like biases in the moral reasoning processes of LMs. This will be operationalized by testing if fine-tuning LMs on fMRI data (of the above-mentioned dataset) helps improve test performance on the ETHICS moral reasoning dataset and if it helps significantly more than just using additional non-neuroimaging behavioural data (moral reasoning permissibility scores) for LM fine-tuning.\n\nMore broadly, this project would fit as a potential proof-of-concept in a new AI alignment research agenda I\u2019m working on, on neuroconnectionism (comparing artificial and biological neural networks) for AI alignment. Moral reasoning is an interesting application area, both for its relevance to AI alignment and because of the availability of public neuroimaging data, as well as e.g. publicly-available LMs fine-tuned for moral reasoning.\nThe non-summary\n\nContext\n\nDuring the last few years, a series of high-profile papers have shown that LMs partially converge towards brain-like solutions and share fundamental computational principles with humans, making them a \u2018biologically feasible computational framework for studying the neural basis of language\u2019. To the best of my awareness though, none of these have explored the potential LM-brain similarities for linguistic inputs rich in morally-relevant content (e.g. moral scenarios), nor tried to improve LM moral reasoning using moral reasoning neuroimaging datasets (though similar ideas have been explored for LMs more broadly and e.g. for Convolutional Neural Networks -CNNs- performing object recognition). \n\nProposal\n\nThe goal of this project is to try to show some transfer, when fine-tuning LMs, between the following moral reasoning neuroimaging (fMRI) dataset and the ETHICS moral reasoning dataset (and, to be more precise, some additional transfer from using the fMRI dataset rather than just from using the behavioural permissibility moral scores part of the fMRI dataset).\n\nAs will be detailed below, this could be useful for reducing risk from AGI/TAI by potentially leading to insights about how human moral reasoning functions mechanistically and by providing evidence about the feasibility of using a new process-based kind of supervision based on neuro-imaging data.\n\nThe major steps involved in this research could look roughly as follows, depending on how much work might be required to show any positive transfer between the moral reasoning neuroimaging (fMRI) dataset and ETHICS: \nneuroimaging moral reasoning dataset data preprocessing (if necessary) / use already preprocessed data in alternative format / use the dataset as processed by a model pretrained on multiple fMRI datasets (including our moral reasoning fMRI dataset)\niterations of LM fine-tuning and result analysis:\ne.g. start by trying to fine-tune a simple, relatively small LM (e.g. just pre-trained BERT; please see fig. 1 from \u2018Inducing brain-relevant bias in natural language processing models\u2019 for an example of what this process might look like, when e.g. fine-tuning the LM to predict the entire fMRI signal - all the voxels) and just check that fMRI fine-tuning helps significantly more than only using the behavioral recordings (how permissible the study participants found the different moral scenarios), measured through the transfer learning improvement made to the ETHICS dataset \ndepending on how good the obtained results are, we can iterate by fine-tuning closer to SOTA / larger / moral reasoning fine-tuned LMs and also by using the fMRI data in other / more creative ways (e.g. parameter-efficient fine-tuning variants, using neural data regularizers, only selecting the voxels corresponding to certain brain regions that are known to be involved in moral reasoning processes in the brain - instead of using the entire brain, etc.)\n\nSome analogies / intuition pumps for the proposal:\nlike feature distillation, with teacher = human in fMRI\nlike process-based supervision, where process = what is measured by the fMRI (like the intermediary steps of an algorithm); major potential advantage over more traditional process-based supervision: could, in principle, also capture implicit knowledge, that experts can\u2019t put into words (e.g. how does someone recognize an object? Or how do they intuitively judge a situation as morally [im]permissible?)\nlike imitation learning with access to internals/intermediary steps of what the expert is doing (e.g. similar to procedure cloning)\n\n\nIn the following, I will provide some additional considerations in a Q&A format.\n\nWhat\u2019s the most ambitious version of this project? \n\nThe most ambitious version of this project would maybe look something like obtaining a state of the art result on [a portion of] the ETHICS dataset [and potentially showing additional transfer and good performance on other moral reasoning datasets], resulting in convincing [publishable in top ML conference] proof-of-concept that process supervision using neuroimaging data seems already feasible; additionally, this might also imply that maybe even collecting neuroimaging data specifically for alignment purposes should be considered. This kind of success might also spur interest from alignment community and other communities (e.g. neuroscience) in extending the neuroconnectionism research agenda to AI alignment-relevant domains, e.g. moral reasoning or pro-sociality.\n\nIf the project succeeds, how would this be useful for reducing risk from AGI/TAI? \n\nThe current version of the idea is focused on human-like moral reasoning both as a proof-of-concept for a task which should be very relevant for AI existential safety (AIs which do moral reasoning in a human-like way could be very helpful) and for opportunistic reasons (existence of publicly accessible neuroimaging datasets for moral reasoning with linguistic inputs, as well as -though less crucially- of decent-sized LMs fine-tuned to do moral reasoning).\n\nThe success of the project could serve as a proof-of-concept for applying the neuroconnectionism (comparing artificial and biological neural networks) research agenda to AI alignment; e.g. it could help lead to insights about how human moral reasoning functions mechanistically and provide evidence about the feasibility of using a new process-based kind of supervision (based on neuroimaging data) for fine-tuning [SOTA] AIs. \n\nWhat\u2019s the least ambitious version of this project? \n\nIn the least ambitious version of this project (especially in case we can\u2019t make the fine-tuning work), we would write an Alignment Forum post describing the experiments and results. One potential interpretation if this project weren\u2019t successful could be that it might be hard to use currently available neuroimaging data for alignment processes [e.g. process supervision using neuroimaging data]; with that in mind, though, I think even such a negative result would be interesting with respect to value of information.\n\nWhat can go wrong, and what\u2019s the backup plan if that happens? \n\nIt might be difficult to show positive gains from the use of fMRI data (compared to just behavioral data - moral permissiveness scores from participants). On the one hand, we will experiment with different Machine Learning techniques to maximize the probability of this project being successful, e.g. by using  parameter-efficient fine-tuning methods or by optimizing the match between the representation similarity matrix of an intermediate LM layer and that obtained from the neural recordings (similar idea applied to Convolutional Neural Networks).\n\nIt might also be hard to have confidence in how well the fine-tuned LM works [e.g. generalisation to other datasets] or how much it\u2019s the fMRI data that helps [for process supervision], but I expect e.g. statistical testing / testing on other datasets to help with any such potential issues.\n\nWhy would anybody expect this project to have a non-trivial chance of working? \n\nOne intuition for why/how this could work: language processing is already necessary for linguistically expressed moral scenarios; LMs already seem to partially converge towards a brain-like solution for language generally, e.g. https://www.nature.com/articles/s42003-022-03036-1  https://www.nature.com/articles/s41593-022-01026-4 (though not tested yet for morally-rich language, to the best of my awareness). It seems very plausible to me that, on the one hand, this similarity would also hold when it comes to text describing moral scenarios, and that it might be [at least slightly] improved by fine-tuning, even without very large datasets. As intuition for the last part, consider that LMs often only require just a bit of fine-tuning for specific tasks (including for moral reasoning tasks, e.g. in https://arxiv.org/pdf/2008.02275.pdf). \n\nAs an additional intuition for the feasibility of this project, previous work has already shown that the features that humans seem to use when computing image-based aesthetic preferences can also be decoded from ImageNet-pretrained Convolutional Neural Networks (as well as from human brains). It doesn't seem like a huge leap to me that this could also be happening with LMs on moral reasoning, and that this could be further improved by fMRI fine-tuning and could in turn help improve transfer to other moral reasoning datasets.\n        \n        \n        OVERVIEW STRUCTURE: I. Introduction A. Background on Language Models (LMs) and Neuroconnectionism B. Overview of Moral Reasoning Neuroimaging (fMRI) Dataset and Project II. Proposal A. Goal of Project B. Steps Involved 1. Data Preprocessing 2. Iterations of LM Fine-Tuning and Result Analysis C. Potential Benefits of Project III. Analogies/Intuition Pumps A. Feature Distillation B. Process-Based Supervision C. Imitation Learning IV. Versions of the Project A. Least Ambitious Version 1. Description of Project 2. Potential Interpretation B. Most Ambitious Version V. Usefulness for Reducing Risk from AGI/TAI A. What Can Go Wrong 1. Difficulty Showing Positive Gains 2. Difficulty Confidence in Fine-Tuned LM B. Why Would Anybody Expect This Project to Work 1. Intuition for Language Processing 2. Intuition for Feasibility of Project VI. Conclusion A. Summary of Project B. Implications of Project\n\n\n        SUMMARY:", "parentIds": [], "childrenIds": ["4556404374076979666", "-5157884108316222980", "7112111640418643767", "-3028441207615448526"], "lastVisited": "1676889918", "group": "path"}, {"id": "4556404374076979666", "text": " This research project aims to use neuroconnectionism, a combination of language models and neuroimaging, to improve artificial general intelligence (AGI) alignment. Specifically, it proposes fine-tuning language models withmoral reasoning neuroimaging (fMRI) data to induce more human-like biases in the moral reasoning processes of LMs. The project seeks to prove a proof-of-concept", "parentIds": ["-4295577623074310743"], "childrenIds": [], "lastVisited": "1676889918"}, {"id": "-5157884108316222980", "text": " This project aims to bridge the gap between Artificial General Intelligence (AGI) and human values by employing neuroconnectionism, a technique whereby language models (LMs) are trained using neuroimaging data. The project proposes to fine-tune an LM on a moral reasoning fMRI dataset to test if this method could improve the transfer learning performance of the LM. Additionally, the project could provide insights", "parentIds": ["-4295577623074310743"], "childrenIds": ["-5632225578292810815", "-2585686465050713748", "5351115991333606699", "-2852817200431662313"], "lastVisited": "1676889918", "group": "path"}, {"id": "7112111640418643767", "text": " This paper proposes a project to use language models fine-tuned on moral reasoning neuroimaging (fMRI) data to improve AI safety. The main goal is to test if fine-tuning the language models on this dataset can improve their performance on the ETHICS moral reasoning dataset. The project involves preprocessing the fMRI dataset, iterative fine-tuning of the language models, and", "parentIds": ["-4295577623074310743"], "childrenIds": [], "lastVisited": "1676889918"}, {"id": "-3028441207615448526", "text": " This project is an exploration of the potential of fine-tuning AI with neuroimaging data, using language models (LMs) and neuroconnectionism (comparing artificial and biological neural networks). The goal is to determine the feasibility of using neuroimaging data for process supervision, particularly towards AI alignment and safety. The process involves preprocessing the data, followed by iterations of LM fine-tun", "parentIds": ["-4295577623074310743"], "childrenIds": [], "lastVisited": "1676889918"}, {"id": "-5632225578292810815", "text": " into how experts recognize objects and judge situations morally. The project can be divided into two versions, least and most ambitious. The least ambitious version will involve fine-tuning a simple LM to demonstrate that fMRI fine-tuning helps more than using behavioural recordings. If successful, this project could be useful for reducing the risk of AGI/TAI, potentially leading to the development of a process", "parentIds": ["-5157884108316222980"], "childrenIds": [], "lastVisited": "1676889918"}, {"id": "-2585686465050713748", "text": " into which brain regions are involved in moral reasoning processes as well as implicit knowledge related to it. If successful, the project could have far-reaching implications for AI alignment safety as it could lead to the development of AIs more attuned to human values. The project could be compared to feature distillation and imitation learning, and has two versions; the least ambitious version involves writing an Alignment Forum post", "parentIds": ["-5157884108316222980"], "childrenIds": [], "lastVisited": "1676889918"}, {"id": "5351115991333606699", "text": " into how humans intuitively judge morally permissible situations. There are two versions of the project - the least ambitious one requiring the LM to make positive gains, while the most ambitious version requires increased confidence in its predictions. The project involves data preprocessing and iterations of LM fine-tuning and result analysis. Successful implementation of the project could spur interest from alignment communities and other fields, such as neuroscience.", "parentIds": ["-5157884108316222980"], "childrenIds": ["-3083639803104818565", "2654737250245558304", "8825818306120475843", "1534063029234314147"], "lastVisited": "1676889918", "group": "path"}, {"id": "-2852817200431662313", "text": " into human moral reasoning. The project has two versions: a least ambitious version that includes data pre-processing, iterations of LM fine-tuning and result analysis, and a more ambitious version which includes intuition pumps and analogue models. If the project succeeds, it could be beneficial in reducing risks from AGI and Transferable Artificial Intelligence (TAI). The success of the project is contingent on demonstrating positive", "parentIds": ["-5157884108316222980"], "childrenIds": [], "lastVisited": "1676889918"}, {"id": "-3083639803104818565", "text": " It could also serve as a tool to reduce risks associated with AGI/TAI.", "parentIds": ["5351115991333606699"], "childrenIds": [], "lastVisited": "1676889918"}, {"id": "2654737250245558304", "text": " It could also provide evidence of the feasibility of using a process-based supervision for AGI alignment by showing similarity between biological and artificial neural networks. The project has the potential to reduce risk from AGI/TAI, potentially helping AI align to human values. In conclusion, this project could prove to be a proof-of-concept for neuroconnectionism, thus aiding AI alignment.", "parentIds": ["5351115991333606699"], "childrenIds": [], "lastVisited": "1676889918"}, {"id": "8825818306120475843", "text": " The project is also notable for its potential to reduce risk from AGI/TAI by providing evidence about the feasibility of using a new process-based kind of supervision based on neuro-imaging data. It is also plausible that a similar process might be happening with moral reasoning, and that it could be further improved by fMRI fine-tuning. In conclusion, this project has the potential to", "parentIds": ["5351115991333606699"], "childrenIds": [], "lastVisited": "1676889918", "group": "path"}, {"id": "1534063029234314147", "text": " Furthermore, the project could serve as a proof-of-concept for applying neuroconnectionism to AI alignment and provide evidence about the feasibility of using neuroimaging for process-based supervision for fine-tuned LMs. In conclusion, this project is a valuable effort in the direction of understanding human values and bringing them closer to AGI, and could also be used to reduce risks from AGI", "parentIds": ["5351115991333606699"], "childrenIds": [], "lastVisited": "1676889918"}], "edges": [{"from": "-4295577623074310743", "to": "4556404374076979666", "relation": "parentId"}, {"from": "-4295577623074310743", "to": "-5157884108316222980", "relation": "parentId"}, {"from": "-4295577623074310743", "to": "7112111640418643767", "relation": "parentId"}, {"from": "-4295577623074310743", "to": "-3028441207615448526", "relation": "parentId"}, {"from": "-5157884108316222980", "to": "-5632225578292810815", "relation": "parentId"}, {"from": "-5157884108316222980", "to": "-2585686465050713748", "relation": "parentId"}, {"from": "-5157884108316222980", "to": "5351115991333606699", "relation": "parentId"}, {"from": "-5157884108316222980", "to": "-2852817200431662313", "relation": "parentId"}, {"from": "5351115991333606699", "to": "-3083639803104818565", "relation": "parentId"}, {"from": "5351115991333606699", "to": "2654737250245558304", "relation": "parentId"}, {"from": "5351115991333606699", "to": "8825818306120475843", "relation": "parentId"}, {"from": "5351115991333606699", "to": "1534063029234314147", "relation": "parentId"}], "name": "test", "pathNodes": ["-4295577623074310743", "-5157884108316222980", "5351115991333606699", "8825818306120475843"], "focusedId": "8825818306120475843"}