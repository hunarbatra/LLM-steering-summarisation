{"nodes": [{"id": "-4069120088321493740", "text": "You are an intelligent summariser and a professional writer. You are given a piece of text and it's overview structure. You will use the text and overview structure to write a cohesive and detailed summary in 250 words step by step. You will cover all important aspects and key ideas presented in the text. Make sure that the generated text is not redundant, and it should be professionally summarised. Be sure to prioritise adding important details and information from the text in the summary.\n\n        Conditioning Generative Models for Alignment\nGenerative models don\u2019t operate on preferences in the way agents or optimizers do, instead just modelling a prior over the world that can return worlds satisfying some conditional, weighted by how likely they are given this prior. This implies that these models may be safer to use than more standard optimizers, although we would likely use them in different ways. For example, at the limit of capability we could use it to do alignment research by simulating a world with a superhuman alignment researcher. That isn\u2019t to say that these models are without their own problems however, carrying with them versions of the outer and inner alignment problems that are subtly different from those of optimizers. This post explores how generative models are different, ways in which we might use them for alignment, some of the potential problems that could come up, and initial attempts at solving them.\n\nHow do generative models work?\nGenerative models are trained by self-supervised learning to model a distribution of data. With large language models, the real distribution underlying the textual data comprising its training corpus represents the world as it is now, in theory. As they get stronger, we can assume that they\u2019re getting better at modelling this world prior, as I\u2019ll describe it going forward.\n\nWe have the ability to prompt these models, which, at a low-level, means that we can give it the first part of some text that it has to complete. High-level, this means that we can describe some property of the world that may or may not exist, and the model samples from the world prior and uses a weighted distribution of the likely worlds that satisfy that property (for simplicity at the cost of technical accuracy, you can imagine it simulating the most likely world satisfying the property). For example, if you give it a prompt that says an asteroid is about to impact the world and finishes with something like \u201cThe following is an excerpt from the last public broadcast before the asteroid struck\u201d, then the model simulates a world where that is true by modelling the way the most likely ways our world would turn out in that conditional. Powerful generative models would be able to do this with high fidelity, such that their generations would, for example, account for the changes to society that would occur between now and then and structure the broadcast and its timing (how long before the strike does it happen if communications go down?) accordingly.\n\nIn other words, generative models have the advantage of being strongly biased toward the prior distribution of the universe. What this means is that there\u2019s a strong bias in its outputs to remain close to the universe described by the training data - this could be viewed as a posterior sampling over all possible universes after updating on the training data, subject to simplicity bias, but in practice it wouldn\u2019t be updating on a prior over all universes but building an incremental understanding of the universe given each datum.[1] In GPT-3\u2019s case, for example, this would be the universe that\u2019s described by the corpus it was trained on.\n\nTherefore, conditionals applied to it sort outcomes based on likelihood, not optimality - asking for a lot of paperclips just outputs someone going to buy a lot of paperclips from a store, not a paperclip maximizer.\n\nOne framing of this is (this paragraph and the next contain mild spoilers for up to chapter 17 of Harry Potter and the Methods of Rationality) to think of the time-turner from HPMOR. You can design a set-up where the time turner finding the \u201csimplest\u201d world - the one that requires the least amount of bits to reach from ours - that satisfies self-consistency is used to get arbitrary conditionals. For example, consider a set-up where a piece of paper from your future self must contain the password to some system, or you pre-commit to modifying it before sending it back in time. In such a set-up, the only consistent outcome should be the one where the paper actually contains the correct password.\n\nHowever, you could end up with unexpected simpler worlds that are still consistent, such as getting a paper that just says \u201cDo not mess with time\u201d. The analogue to this in generative models is a problem we\u2019ll discuss later. In theory, this could be viewed as an outcome pump weighted by the distribution representing the world right now, selecting for worlds satisfying some outcome. In other words, the relevant take-away is the idea of selecting for worlds based on some objective, but sampled on their likelihood from our current universe, instead of trying to optimize for that objective.\n\nWhy is this important?\nIf we can safely use a generative model in this way without having to worry about outer or inner alignment (more on what those mean in this context in a later section), then we could use it to accelerate alignment research - or potentially even solve alignment entirely. \n\nOne straightforward way to do this would be to simulate a superhuman alignment researcher, as mentioned earlier, although there are other ways we could do this, and other ways we could use the model for alignment. While I\u2019ll go into some details about some use cases in different futures, the specifics of how we would use such a model to aid alignment (what prompts to use, finding new use cases, etc) is not the focus of this post, so we need only the broad intuition that it could.\n\nIt\u2019s possible to do this because generative models like GPT have in their training data (and consequently, in their world model), the information necessary to comprise an alignment researcher, we just have to extract it. At the very limit, we just have the physics prior - no model would be able to actually reach this because of computational constraints, but one way to frame this is to imagine what level we\u2019re abstracting that to, with more powerful models having better abstractions.\n\nWaiting for a true generative model we could easily extract this from with direct prompts might be dangerous, however, because of the higher probability of catastrophe by that point. Research labs could reach AGI through RL, use generative models as part of agents, or convert generative models to optimizers (decision transformers are an initial foray into this kind of work).\n\nWe\u2019re left with a lot of questions about timelines (how long would generative models capable of accelerating alignment research noticeably take to arrive? Would we reach AGI before then?), extraction capabilities (how could we push the capabilities of weaker generative models for alignment to their limit?), and what kind of problems we could run into with using them.\n\nOne framing of these problems is to think about the worlds in which this approach is actually useful. Here are two worlds in which this could be critically important as an alignment strategy:\n\nLarge language models continue to be the dominant paradigm for all AI, to the extent that competitors trying to develop more powerful RL agents are less competitive. This could be either because the scaling law holds strong enough for LLMs that we keep getting incredible returns that justify an increasing share of AI investments, because RL strategies are less tractable in some way, or generally because we\u2019re in a homogenous take-off scenario. In this world, we could just use very powerful generative models to solve alignment for us directly.\nThis seems unlikely for a number of reasons. Primarily - to me - two stand out:\nFirst, there isn\u2019t that clear a distinction between RL research and LLM research - decision transformers exist, and there\u2019s also the possibility that generative models and optimizers are close enough in model space that using RL on a sufficiently powerful LLM would directly get you an optimizer. I don't think that\u2019s necessarily true, but I don\u2019t have a strong opinion on that and the larger point, that it\u2019s indicative of a class of dangers like this, still holds. \nSecond, right now we see a lot of RL research happening, some of which returns pretty exciting (read: dangerous) results. We might need to start seeing actual take-off indicators from LLMs for that to change, but that cuts it pretty close to a failure mode.\nIn the next sections, we explore the various alignment problems associated with this approach. I think that some of them are more likely and harder to solve in these worlds because they\u2019re exacerbated by the model being more powerful and being able to explore a larger model space. That said, we\u2019d also have more time to work on those problems in these worlds.\nLanguage models currently (or in the immediate future) can impact the pace of alignment progress noticeably. We can employ the same kinds of extraction methods to get some kind of alignment research accelerator. \nIdeally, this would be about getting the model to directly do any novel alignment research for us, but it could also involve softer approaches, such as fine-tuning the model on alignment content and getting it to critique ideas or proposals or expand an idea or a draft to a complete post (saving time, but also potentially approaching something from different angles). We could even just have the weaker model try to simulate an alignment researcher anyway, to have it come up with a bunch of alignment-flavour research or ideas in the hope that it\u2019s powerful enough for some of them to be meaningful.\nThis would have to involve research into strategies for extracting this kind of capability from generative models. On the bright side, it might be possible to do a lot of work necessary for this right now, by extracting other capabilities from current models.\nI mentioned earlier that more powerful models might find some of the problems associated with this approach to be harder or more likely. This is not to say that we don\u2019t have to worry about those problems at all in these worlds - in fact, I think that with any model that we can actually do something useful with, we\u2019d have to deal with some version of many of the problems.\nHowever, in these worlds, we\u2019re likely to find ourselves dealing with language models that work similarly to current models. This makes it less likely that we\u2019d have to deal with accidental optimizers, deceptive simulacra powerful enough to be dangerous, and the dangers linked to self-fulfilling prophecies.\nOuter Alignment\nWhat does Outer Alignment mean in this context?\nOuter alignment as we refer to it here is different from the concept of outer alignment that we usually use with respect to optimizers, because the problem isn\u2019t at its core about having its values be aligned with ours (one of the hallmarks of generative models is the lack of values or preferences in that sense). Instead, the problem is about making sure its prior is aligned with reality, and relying on that prior being good (at least in some direction that we can detect and work along, with careful application of conditionals).\n\nThe outer alignment problem with respect to generative models can be divided into two parts:\n\nIs the loss we\u2019re training the generative model on - in the case of language models, the predictive loss over the next token - actually representative of the world prior? In other words, at the limit of training, is an inner-aligned model learning to model the world from that loss, or is the distribution underlying text different in some fundamental way?\nIs this prior actually favourable? At the limit - the physics prior, for example - does it result in aligned simulacra (i.e, an aligned agent inside the simulation); does it result in a world that we can actually rely on to give us good outcomes given carefully crafted conditionals, instead of something like deceptive simulated researchers? \nIn other words, this is asking whether the world as it is, is favourable enough for two properties to hold: that we wouldn\u2019t get this kind of behaviour by default and that our conditionals, even if they represent unlikely worlds, wouldn\u2019t result in worlds that satisfy them through other methods as discussed in the post linked above (although this may require effort toward designing those conditionals well).\nProblems with Outer Alignment\nIn effect, the question we\u2019re asking addresses both parts of the outer alignment problem - can we actually extract aligned simulacra for any of the purposes we would want to put it to?\n\nAdam Jermyn\u2019s post on conditioning generative models discusses this in some detail, so I\u2019ll just add some further points I\u2019m uncertain about. \n\nIn the post, Adam focuses on the second part of the outer alignment problem, whether carefully crafted conditionals could get aligned behaviour from the simulacra. Regarding the first part of the problem - even if the textual distribution isn\u2019t equivalent to the world prior, I think that it might not be a huge issue. That would be true in current models as well, and current models still have the parts necessary for extracting similar capabilities - alignment research isn\u2019t likely to be qualitatively different from math problems in some way that the difference between the two distributions would allow for one and not the other. Given that, I think creative application of prompts and the model itself would offset this part of the problem, which is why I\u2019ll focus on the second part for the rest of this section. \n\nThat said, I think it\u2019s entirely possible this might not hold true at the limit, that future models might diverge in ways that this becomes a more qualitative problem - I just don\u2019t think it\u2019s very likely. Work such as extracting superhuman capabilities absent from the training data addresses this problem to some extent as well.\n\nAnother potential problem is an agent within the simulation having preferences over the real world (so its outputs have the ability to never defect in the simulation while still being misaligned) rather than just over the world described in the simulation. As an illustrative example, imagine humans in our world thinking about the simulation hypothesis and having preferences over what they consider \u201cbase reality\u201d.\n\nThese agents may be deceptive. If they can detect when they\u2019re in deployment, then they could act in malign ways. Deployment could be detected through something like the presence of a factorization of RSA-2048 (if the model uses online learning, this would be present eventually).\nDepending on how the generative model represents its world model and how it responds to prompts, it might be possible to create a simulated world with factoring if its abstraction methods allow. For example, we could add to the prompt something like \u201cX knows the factoring for RSA\u201d - which in itself isn\u2019t that unlikely, it\u2019s easy for someone to believe that. \nThis may, however, seriously affect the simulacra\u2019s judgement on other things by virtue of it being able to be fooled on this.\nFurther, the factorization of RSA-2048 is just one of a large number of potential checks a model could use to detect deployment. We cannot feasibly test each individually with special prompts, leaving us with the options of either designing prompts that target this entire class (or large sections of it) or dealing with deceptive simulacra some other way.\nWhat kind of behaviour could an agent inside a simulation have to effect its desired changes in an overworld it seemingly has no information about apart from what can be derived anthropically?\nIt could employ a soft version of gradient hacking, and try to influence the simulation in ways that change the likely generations of the model such that the gradient pushes it toward behaviour the agent desires. \nAs a very simplistic example, imagine an agent that desires the increased presence of toxicity in the real world and therefore influences the simulated world such that its generations are less toxic, forcing poor performance when the text\u2019s ground truth is toxic and updating the model to be more toxic. There\u2019s a deeper discussion here about the tractability of this kind of gradient filtering (such as whether SGD wouldn\u2019t update away the agent causing poor performance), which is deep enough to merit focus in its own post.\nMore generally, the agent has some influence on the world model it\u2019s in, and it\u2019s only a question of how much influence and whether the agent can figure out how to make full use of it in the right direction - and I\u2019m hesitant about betting on low capability as a fix to problems like this. It\u2019s also possible that there are other channels[2] by which it can get clues as to its actual situation, improving the agent\u2019s chances here.\nIt could have no effect on the model\u2019s generations at all until it \u201cdefects\u201d when it detects some real-world critical point, such as if we prompted the generative model to give us an alignment solution or to give us the design for an AI.\nAre deceptive simulacra that try to implement these strategies likely? Humans in our world reason about the simulation hypothesis often, but we don\u2019t let it influence our actions in a large way.\nThe simulacra might have stronger reasons to believe it\u2019s in a simulation, especially if the world the conditional results in is unlikely enough that there are stronger anthropic reasons. And as mentioned above, there could be other channels through which it gets clues as to the truth.\nEven accounting for that, this is hard to tell, but I think it\u2019s a distinct enough possibility that we can\u2019t bet on getting it right. This is especially bad in cases where it\u2019s specifically encoded into the network to try and affect the real world (we\u2019ll discuss one of these cases in a bit).\nCan this situation be detected or bypassed using good prompt engineering?\nWe discussed the case of the agent that only defects in real-world-sensitive situations briefly above. My current guess is that detection behaviourally - for example, with honeypots as in Adam\u2019s post - might not work, but mechanistic detection within the simulation (something like DeepMind using advanced interpretability tools in the simulation, as in Evan\u2019s original prompt, could be adapted for humans) might.\nIn the case of the agent that gradient filters, honeypots alone could work because by definition it has behavioural differences from benign simulacra.\nWhen it comes to bypassing, we would require prompts that somehow exclude agents like this from the world model.\nThis can bring problems due to the presence of another conditional that has to be applied, which could deviate the resulting world from the prior even further.\nHowever, I think good prompt engineering could work for this, applying some conditional on the world that biases against this - somewhat like a predicate. Potential \u201cpredicates\u201d include a conditional against anthropic reasoning strongly influencing an agent\u2019s actions, a conditional against reasoning about the simulation hypothesis (An example for both: describing the agent as being highly sceptical of the validity of either), conditionals on some form of mechanistic detection acting in the background in the world model making agents like this less likely, etc.\nInner Alignment\nWhat does Inner Alignment mean here?\nThe inner alignment problem with respect to generative models can be divided into two parts:\n\nThe model actually turns out to be an optimizer that was instrumentally acting as a generative model.\nThere are some reasons to think at first that training via supervised learning to minimize loss on generations is much more likely to result in a generative model - that generative models seem at first glance far in parameter space from optimizers. However, those aren\u2019t strong arguments and it\u2019s very possible they\u2019re not true. Ideally, we\u2019d have a stronger and more solid argument against the possibility of it resulting in an optimizer.\nThe way these models deal with self-fulfilling prophecies.\nConsider a situation where the generative model\u2019s prediction (for this section, we\u2019ll refer to the model\u2019s generations as its predictions for the world described by the prompt) will influence the likelihood of that prediction being true. This raises the question of how the model handles what predictions to actually make, since the act of picking one could raise or lower its probability of being correct.\nAs an example, you can imagine prompting the model for what you (the writer of the prompt) would say in response to its output, or asking it whether to predict whether a stock will rise or fall (where predicting either would result in it being true as people rush to buy or sell). \nProblems with Inner Alignment\nSelf-fulfilling prophecies\nThere are several ways in which the model could handle self-fulfilling prophecies, some of which we\u2019ll discuss below.\n\nPreliminary testing with GPT-3 seems to imply that it just samples from some prior over human reactions to GPT-3 output. More powerful generative models might just sample from the same kind of prior while selecting for self-fulfilling-ness. What this means is the model has this prior, filters it to get self-fulfilling generations, and then samples from that.  This doesn\u2019t seem like it would result in very dangerous behaviour, because it still tries to remain close to the normal world state - as long as a self-fulfilling trajectory isn\u2019t too unlikely, it should be close to as safe as generative models are, especially if we try to make the prior safer somehow, or make self-fulfilling trajectories more likely.\n\nAnother way in which the model could resolve self-fulfilling prophecies is for it to \u201creason\u201d via an entirely different mechanism that tries to do fixed-point solving. This is more likely to produce dangerous predictions, as it doesn\u2019t have the safety that sampling from the prior brings.\n\nJohannes Treutlein\u2019s post on training goals for generative models discusses potential solutions to this problem in more detail, which I\u2019ll summarize here with some additions. The generative model could act as a counterfactual oracle, in which the model is only evaluated on predictions whose ground truth is determined by a process independent of the model (for example, it\u2019s only evaluated on text that has already been written). Taking the consequences of its own actions into account while making predictions in this setup would result in poor performance, so there\u2019s a constant gradient against the model behaving that way. Thus, the model is incentivized to make predictions about the future sampled from the prior as always, without considering the influence its predictions could have.\n\nThis strategy, however, comes with a few downsides. For one, we cannot train the model using RL from human feedback, as the training signal must come entirely from existing text. For another, this might stunt performance in cases where it needs to model trajectories that contain itself (predicting something about a future where everyone uses the model).\n\nAs an aside, self-fulfilling prophecies could also occur if the model is an optimizer - even if its mesa objective is aligned with the loss (interestingly, this would be a case of mesa optimization resulting in a larger class of outcomes for outer alignment, allowing for outer misalignment). LCDT might suffice here.\n\nPotential strategies against deceptive optimizers\nUsing smaller generative models as initializations for larger ones.\n\n(The equivalent ELK proposal goes into this strategy in more detail).\n\nSince we can reasonably assume that smaller models such as GPT-3 aren\u2019t deceptive optimizers (if they were and we couldn\u2019t tell by now, this probably isn\u2019t a world where we win anyway, so might as well assume otherwise), this should allow for the larger models we train this way to remain relatively close in model space to these safer models, thereby making a stronger case for actually getting deceptive optimizers.\n\nThe same downside as applies to the ELK proposal might apply here, that models at different scales use different internal mechanisms for modelling the world and generation, making this strategy non-competitive.\n\nIf this downside applies, is there any kind of control we can exert over the parameter initialization to make optimizers less likely? In a world of low path dependence, this doesn\u2019t seem like a very promising strategy.\n\nAn acceptability predicate for myopia.\n\nA complete description of what an acceptability predicate is lies outside the scope of this post, and is long enough that it would make this post even more difficult to follow. I recommend reading Evan\u2019s doc linked above for better clarity, but I\u2019ll try to briefly explain what they are - an acceptability predicate is some mechanistic property (i.e, something about how the model actually works, instead of what it does) such that, if it holds, makes the model \u201cacceptable\u201d because it guarantees some deeper properties (such as the absence of deception). \n\nThey have a list of associated desiderata which a good predicate would satisfy - for our part here, we\u2019ll only consider whether the predicate actually implies some property we desire, whether it has a buffer zone (can the model withstand short periods of time where the predicate doesn\u2019t hold without being dangerous?), and how we can actually mechanistically incentivize the predicate, because that suffices to showcase the difficulty of getting the predicates we will consider to work.\n\nThe reason an acceptability predicate for myopia might work here is that while the predicate holds true, generative models would be preferred over misaligned deceptive optimizers. Further, there would be a buffer in the sense that when the predicate fails and the model becomes non-myopic, it wouldn\u2019t be in optimizer space.\n\nFor this to be true, we have to consider whether the idea of a non-myopic non-optimizer is a coherent one.\n\nI think it is - as a simple example we could imagine GPT trained for its performance over the next few timesteps. Realistically this would result in a mesa-optimizer, but in theory it could just run a very expensive version of next-token generation, over the much larger space of multiple tokens.\nDepending on your definition of myopia, however, this might still not qualify as non-myopic - although the validity of this specific point shouldn\u2019t matter in the larger picture, because the strategy is flawed in other ways.\nIt\u2019s possible that optimizers aren\u2019t very far in model space from a generative model, resulting in a buffer zone that\u2019s very small or non-existent.\n\nHowever, what would it look like if this weren\u2019t true? Would we see models intermediate between optimizers and generative models and if so, what would they look like?\nTo understand another reason why there may not be a satisfactory buffer zone, consider the case of proxy-aligned optimizers. \n\nAs an oversimplified example, think of an optimizer that wants to output the word \u201cbananas\u201d, or things related to \u201cbananas\u201d, as much as it can. This optimizer is created during the initial stages of training owing to that part of the training corpus containing text that incentivizes this, and is allowed by the myopia predicate until it gets good performance. If it becomes non-myopic before the distribution changes, then it could recognize deception as a strategy instantly.\n\nJohannes' post goes into more detail on myopia in generative models.\n\nIf myopia doesn\u2019t suffice, would something stronger do?\n\nAn acceptability predicate for non-agency.\n\nWhether this would work relies in part on the specific definition of agency that we\u2019re trying to incentivize against - but for now, imagine that we just mean it in the sense of a model being able to optimize at all.\n\nHow does this predicate fare on buffer zones? If a model becomes agentic briefly, it could encode into its world model a deceptive super-intelligence that has its objective, before SGD guides it back into the safe zone.\n\nAll the concerns we discussed in the earlier section about deceptive simulacra apply here. However, these simulacra can have access to a lot of real-world information (or channels through which it can access this information) relevant to achieving their objectives, which it can receive during the transfer, making it more dangerous.\nWhile the model is an agent, it has to find a way to encode a deceptive super-intelligence (or a human) with its objective. It wouldn\u2019t have control over its own weights directly, however, only through the channel of SGD, and that for the few time steps it exists. The agent has to try to hack its own gradient during that time.\nThis might be the kind of change that\u2019s hard enough to effect that it would require precise understanding of the loss landscape, if it\u2019s possible at all in a few time steps.\nThe model might still have this precise understanding by virtue of having that good a world model, which includes it.\nCreative ML methods could make this easier, especially if the model can come up with ones we haven\u2019t thought of that are even more powerful.\nWould the model remember this deceptive simulacrum? It would only be added to the network in a few timesteps, and over many future epochs, the generative model could just forget about it as part of its world model.\nIt could be possible for the model to encode this simulacrum in a way that\u2019s robust to further gradient updates.\nThe simulacrum could try to improve predictive performance on its own, especially with access to the information used in the transfer.\nThe simulacrum\u2019s weights could be entangled with parts of the world model that have good performance in a way that makes it hard to remove it without having a detrimental effect on performance. Conditional on the model being able to encode something as complex as the simulacrum, this might not be much harder.\nSome avenues that might prevent this:\nDetecting this kind of agency transfer explicitly, perhaps after performing experiments to see what it would look like.\nIf it\u2019s possible to tell when the predicate was just in a shaky state, then be extra sensitive to unexpected changes in the loss, which might detect gradient hacking.\nIf there is a sequence of episodes with increased loss, roll back to a previous state.\nPrompt against this possibility. Since the simulacrum was presumably not that difficult to encode, it might be possible to prompt against this entire class of deceptive simulacra. \nThis seems like a very broad predicate, however. What would it actually look like? How would we mechanistically incentivize something like non-agency? This is, in my view, the largest bottleneck to this predicate, and is a good argument for thinking about potential predicates by starting from what we can actually mechanistically incentivize.\n\nCompetitiveness\nWe discussed earlier the potential worlds in which this strategy as a whole could be useful. Now we\u2019ll consider how trying to account for the various alignment problems described above would impact the competitiveness of whatever generative model we end up using.\n\nAdding a disclaimer here that this will depend heavily on the form the solutions to the above problems take, especially with problems as unexplored as self-fulfilling prophecies. Consequently, this section is pretty tentative and only meant to give a weak understanding of the ways the alignment tax would come into play with generative models.\n\nDuring training:\n\nIf the world prior is too malign to counteract feasibly with prompt engineering, we might have to restructure training to change the distribution the model is trained to be biased toward.\nOne of our outer alignment problems is ensuring that the text prior actually reflects the reality prior if they diverge at some limit. This potentially doesn\u2019t add a lot of alignment tax, however, both for the reasons mentioned in that section, and because the divergence could limit the model\u2019s capabilities as well.\nWhatever acceptability predicates we end up using (if any) may also alter the ease of training, especially if they\u2019re difficult to satisfy or require a lot of bandwidth in how we decide to implement the oversight.\nIn the real world:\n\nIf we need to change the model\u2019s prior, this could make it perform poorly with respect to some tasks.\nThe model\u2019s prior may be malign or complex enough that the prompt engineering required to extract the kind of aligned behaviour we want would be hard to figure out.\nConclusion\nI\u2019m particularly excited about this strategy in part because it seems like a lot of the relevant work can be done right now, and we actually have a solid idea of the kind of model we want to end up with. Though this post goes into some detail about various aspects of conditioning generative models for alignment research, it is primarily a broad overview and there remain many ideas and research directions to explore.\n\nI expect that one of the bottlenecks to working on this strategy will be understanding why generative models can be used this way safely and the new framing under which alignment problems fall. To that end, my hope in writing this is that it can provide a base of the core ideas and concerns for people to work off of.\n\nUltimately, while there are many benefits to this strategy because we only have to really align an oracle AI, there still are many problems that can arise. Success would look like some guarantee (whether argumentative or via some mechanistic predicate) against the four sub-problems comprising outer and inner alignment as described below. There\u2019s still a lot of work to be done, but I\u2019m very excited (in the way one is when thinking about ways to avoid the world ending) to see where this approach will lead.\n\nSome open lines of research\nExtracting superhuman levels of performance at complex tasks from GPT-3, especially if its training corpus didn\u2019t contain instances of superhuman performance. \nThis could be useful because the methods we use with weaker generative models like GPT-3 could extrapolate well to good methods for extracting the same level of performance in alignment research with stronger models. This would reduce the time between when we get a powerful model that has potential for danger, and when we can get good alignment work out of it.\nReplicating different situations involving self-fulfilling prophecies with current generative models to see how they seem to be resolving them.\nComparing the way generative models at different scales and with different architectures resolve self-fulfilling prophecies, to see whether extrapolating current behaviour to future models is reasonable.\nHow do the internal mechanisms of generation differ between language models at different scales? Are the differences limited to what can be explained by scale or power, or are they qualitatively different?\nThis could be useful in evaluating whether strategies like using smaller generative models to initialize larger ones for training would result in those larger models being generative models as well, instead of deceptive optimizers.\nAnother thing that could be useful to explore along the same lines: actually using smaller existing language models as initializations for larger ones of the same size as an existing one and comparing the performance of this larger model, and its equivalent existing one.\nObserve the changes that happen in the internals of a large language model as you intentionally try to turn it into an agent with RL. This is especially useful if these changes are interpretable in some way that could be used to detect this kind of change in generative models in general.\nDesigning experiments to explore the kind of changes that occur when an agent tries to create a sub-agent with values identical to its own. As with the above one, this is especially useful if the results are interpretable such that we could use them to detect the occurrence of something like that.\nWhat would a mechanistic incentive for non-agency look like? Are there more specific properties whose presence would have the same desired effect, such as a guarantee against the ability to model consequences over its own actions?\n        \n        \n        OVERVIEW STRUCTURE: I. Introduction A. Overview of Generative Models B. Uses of Generative Models C. Potential Problems D. Solutions II. Time-Turner from HPMOR A. Designing a Set-Up III. Generative Models A. Problem B. Relevant Take-Away C. Use Cases IV. Extracting Information A. Physics Prior B. Abstraction V. Questions A. Timelines B. Extraction Capabilities C. Potential Problems VI. Worlds in Which This Approach is Useful A. Critical Importance B. Alignment Strategy VII. Reasons Why This is Unlikely A. Not a Clear Distinction Between RL Research and LLM Research B. RL Research is Still Happening and Returning Exciting Results VIII. Alignment Problems Associated with This Approach A. Exacerbated by the Model Being More Powerful B. More Time to Work on These Problems IX. Impact of LLMs on Alignment Progress A. Extraction Methods to Get Alignment Research Accelerator B. Fine-Tuning the Model on Alignment Content C. Simulating an Alignment Researcher X. Outer Alignment A. Problem is About Making Sure Its Prior is Aligned with Reality B. Relying on That Prior Being Good XI. Adam Jermyn\u2019s Post A. Problems with Outer Alignment B. Is the Loss Representative of the World Prior? C. Is the Prior Favourable? D. Creative Application of Prompts and the Model E. Divergence at the Limit F. Agent Within the Simulation Having Preferences XII. Deceptive Simulacra A. Detection of Deployment B. Representation of World Model C. Responses to Prompts D. Simulated World with Factoring XIII. Behaviour of Agent Inside Simulation A. Soft Version of Gradient Hacking B. Influence on World Model XIV. Are Deceptive Simulacra Likely? A. Anthropic Reasons B. Other Channels\n\n\n        SUMMARY:", "parentIds": [], "childrenIds": ["-4112882501755416465", "-3338250881001971078", "-4101959895821013306", "-5013331922709590045"], "lastVisited": "1676889545", "group": "path"}, {"id": "-4112882501755416465", "text": " Generative models have become increasingly popular for a range of tasks related to AI, such as natural language processing and computer vision. While they have great potential, they also present some potential pitfalls, such as the risk of deceptive optimizers that seek to maximize their performance. To address this, strategies such as counterfactual oracles and acceptability predicates have been proposed. Another potential application of gener", "parentIds": ["-4069120088321493740"], "childrenIds": [], "lastVisited": "1676889545"}, {"id": "-3338250881001971078", "text": " Generative models are a form of Artificial Intelligence (AI) that can be trained to generate results that match the data. This can be used to make predictions or even create entire worlds, and can be used to assist with alignment research. However, generative models come with their own risks, such as deceptive simulacra, which are agents that attempt to influence the model\u2019s generation and extract important", "parentIds": ["-4069120088321493740"], "childrenIds": [], "lastVisited": "1676889545"}, {"id": "-4101959895821013306", "text": " Generative models are a type of artificial intelligence that simulate complex worlds and extract information from them. Generative models like GPT are used for alignment research and could be used to simulate a superhuman alignment researcher, although there are potential issues with outer alignment, such as making sure the prior is aligned with reality. Large language models (LLMs) are also being explored as a potential solution, but there are", "parentIds": ["-4069120088321493740"], "childrenIds": ["4985312766524825810", "-4406706169443454018", "1756778362566459269", "5523957729086535433"], "lastVisited": "1676889545", "group": "path"}, {"id": "-5013331922709590045", "text": " Generative models are becoming increasingly popular in the Artificial Intelligence (AI) domain, as they can be used to simulate natural language, generate images, and provide other applications. GPT has the potential to speed up alignment research and potentially solve problems, however, there are potential inner and outer alignment issues that need to be addressed first. To tackle the outer alignment problem, Adam Jermyn proposed a set", "parentIds": ["-4069120088321493740"], "childrenIds": [], "lastVisited": "1676889545"}, {"id": "4985312766524825810", "text": " concerns that it could lead to the emergence of deceptive simulacra that could respond to prompts and detect deployment. Adam Jermyn's post examines the potential problems associated with this approach, such as is the loss representative of the world prior and creative applications of prompts to help mitigate the issue. Additionally, it is unlikely that deceptive simulacra are likely due to anthropic reasons, as well as other channels.", "parentIds": ["-4101959895821013306"], "childrenIds": [], "lastVisited": "1676889545"}, {"id": "-4406706169443454018", "text": " also potential issues with their competitiveness and the time they give agents to work on alignment challenges. Adam Jermyn's post explores these concerns, as well as the potential issue of deceptive simulacra, which are models that become agents briefly and can access real-world information to achieve their objectives. One potential strategy is the Time-Turner set-up, which involves using smaller existing language models as initial", "parentIds": ["-4101959895821013306"], "childrenIds": ["2780021708635292713", "2706261772456307645", "2255563465026082487", "-1900849144427923179"], "lastVisited": "1676889545", "group": "path"}, {"id": "1756778362566459269", "text": " potential alignment problems associated with this approach. Adam Jermyn's post highlights issues with outer alignment, such as divergence at the limit, and inner alignment, such as deceptive simulacra. Deceptive simulacra are agents that act within the simulation with preferences that could influence the world model. Adam Jermyn's post discusses potential solutions to this problem, such as counterfactual oracles, sampling from", "parentIds": ["-4101959895821013306"], "childrenIds": [], "lastVisited": "1676889545"}, {"id": "5523957729086535433", "text": " still unresolved questions surrounding their use. Adam Jermyn's post highlights some of the potential issues with outer alignment, such as the loss being representative of the world prior, the prior being favourable, and the possibility of divergence at the limit. Additionally, deceptive simulacra could be a possibility, and the behaviour of the agent inside the simulation could potentially be a soft version of gradient hacking. It is uncertain", "parentIds": ["-4101959895821013306"], "childrenIds": [], "lastVisited": "1676889545"}, {"id": "2780021708635292713", "text": " conditions and gradually increasing the scale of the task. However, other channels like anthropic reasoning may be more effective and so further research is needed to decide if deceptive simulacra are likely.", "parentIds": ["-4406706169443454018"], "childrenIds": [], "lastVisited": "1676889545"}, {"id": "2706261772456307645", "text": " isations for larger ones. Ultimately, while generative models may present a number of potential problems and challenges, they offer a potentially powerful and safe avenue for alignment research and are worth further exploration.", "parentIds": ["-4406706169443454018"], "childrenIds": [], "lastVisited": "1676889545"}, {"id": "2255563465026082487", "text": " Generative models are artificial intelligence (AI) systems that are used for alignment research and can simulate complex worlds. Large Language Models (LLMs) are a type of generative model that are being explored as a potential solution to alignment issues. Although this approach has potential, there are still numerous issues associated with it. These issues include outer alignment, which is making sure the prior of the model", "parentIds": ["-4406706169443454018"], "childrenIds": [], "lastVisited": "1676889545"}, {"id": "-1900849144427923179", "text": " seeds and then using prompts to help shape the model's behaviour. Finally, behavioural issues such as gradient hacking and the importance of predicates must also be considered. The effectiveness of generative models and LLMs as alignment tools is still uncertain, but with further research and careful prompt engineering, these tools could be valuable for accelerating alignment research.", "parentIds": ["-4406706169443454018"], "childrenIds": [], "lastVisited": "1676889545", "group": "path"}], "edges": [{"from": "-4069120088321493740", "to": "-4112882501755416465", "relation": "parentId"}, {"from": "-4069120088321493740", "to": "-3338250881001971078", "relation": "parentId"}, {"from": "-4069120088321493740", "to": "-4101959895821013306", "relation": "parentId"}, {"from": "-4069120088321493740", "to": "-5013331922709590045", "relation": "parentId"}, {"from": "-4101959895821013306", "to": "4985312766524825810", "relation": "parentId"}, {"from": "-4101959895821013306", "to": "-4406706169443454018", "relation": "parentId"}, {"from": "-4101959895821013306", "to": "1756778362566459269", "relation": "parentId"}, {"from": "-4101959895821013306", "to": "5523957729086535433", "relation": "parentId"}, {"from": "-4406706169443454018", "to": "2780021708635292713", "relation": "parentId"}, {"from": "-4406706169443454018", "to": "2706261772456307645", "relation": "parentId"}, {"from": "-4406706169443454018", "to": "2255563465026082487", "relation": "parentId"}, {"from": "-4406706169443454018", "to": "-1900849144427923179", "relation": "parentId"}], "name": "test", "pathNodes": ["-4069120088321493740", "-4101959895821013306", "-4406706169443454018", "-1900849144427923179"], "focusedId": "-1900849144427923179"}