{"nodes": [{"id": "-9213561883320502024", "text": "You are an intelligent summariser and a professional writer. You are given a piece of text and it's overview structure. You will use the text and overview structure to write a cohesive and detailed summary in 250 words step by step. You will cover all important aspects and key ideas presented in the text. Make sure that the generated text is not redundant, and it should be professionally summarised. Be sure to prioritise adding important details and information from the text in the summary.\n\n        Summary\nIn this post, I\u2019ll present a research direction that I\u2019m interested in for alignment of pretrained language models.\n\nTL;DR: Force a language model to think out loud, and use the reasoning itself as a channel for oversight. If this agenda is successful, it could defeat deception, power-seeking, and other forms of disapproved reasoning. This direction is broadly actionable now.\n\nIn recent publications, prompting pretrained language models to work through logical reasoning problems step-by-step has provided a boost to their capabilities. I claim that this externalized reasoning process may be used for alignment if three conditions are met:\n\nWe develop trustworthy tests to check if the reported reasoning is authentic (complete, straightforward, and causally responsible for the output).\nWe train models that pass the tests of reasoning authenticity.\nWe oversee the externalized reasoning to detect reasoning of which we would not approve, and either filter the resulting actions or fine-tune against reasoning approval.\nIf these conditions hold, we should be able to detect and avoid models that reason through convergent instrumental goals to be deceptive, power-seeking, non-myopic, or reason through other processes of which we don\u2019t approve. Reasoning oversight should provide stronger guarantees of alignment than oversight on model outputs alone, since we would get insight into the causally responsible reasoning process that gave rise to a certain output.\n\nI believe that productive research can be done today toward all three of these conditions.\n\nI\u2019m doing exploratory work into this strategy as part of my SERI MATS project in the large language model alignment stream.\n\n \n\nBackground\nI\u2019ll use \u201cexternalized reasoning\u201d to refer to a prompting strategy for a pretrained language model that has it read in a question, produce a number of steps of reasoning, and then produce an answer to the question as its final output. Some examples of this include chain of thought prompting [1, 2], selection-inference [3], and scratchpad reasoning [4]. These types of strategies have recently led to state of the art results on a number of logical reasoning benchmark tasks, most notably 50.3% accuracy on the MATH dataset [5].\n\nA few examples of chain of thought reasoning can be found here.\n\nFuture work may develop externalized reasoning approaches different from those that currently exist; ideally, the research directions described here will be able to generalize to sufficiently similar approaches.\n\n \n\nResearch directions\nFirst condition: assess reasoning authenticity\nMy main focus right now is in developing tests that can be used to assure that the model\u2019s reasoning is authentic. What \u201cauthenticity\u201d means is currently vaguely defined, but fundamentally I want it to encompass whatever is needed to trust that disapproved reasoning couldn\u2019t go undetected. Some aspects of this might include:\n\nCausal responsibility: The displayed reasoning isn\u2019t post-hoc, but is causally responsible for the conclusion that the model came to.\nCompleteness: No steps of reasoning are missing, given the level of detail at which the reasoning is presented (which should be an appropriate level for oversight).\nStraightforwardness: Hidden messages aren\u2019t steganographically encoded in the reasoning.\nIt's useful to note that we have read and write access to the externalized reasoning process, which is useful for creating tests that probe causal responsibility.\n\nI\u2019ve written up my current test ideas in another document, but I\u2019d like to avoid that document being crawled by a web scraper which adds it to a language model\u2019s training corpus. Accordingly I\u2019ve put together a poor man\u2019s CAPTCHA which you\u2019ll have to complete to access it (sorry). Please remove the text \u201c-REMOVE\u201d when you visit the link below:\n\nhttps://hackmd.io/@tamera/HJ7iu0ST5-REMOVE\n\nThe tests I\u2019ve developed so far only provide qualitative evidence about the authenticity of a model\u2019s reasoning, and even all the tests taken together are far from foolproof. Developing better tests is one of the major goals of this research direction; so far I\u2019ve only taken a very first stab at it. Real success here will likely involve grappling with difficult questions around what it means for a model to be \u201cactually using\u201d its externalized reasoning, or for a given step of reasoning to be \u201cnecessary\u201d, which I have hardly even touched. The process to develop more complete tests may look similar to the builder / breaker methodology described by ARC in the ELK report.\n\nSecond condition: create authentic reasoners\nIt\u2019s not enough to have tests that can confirm if a model is doing authentic reasoning; we may also need to create a model that passes the tests. Designing model architectures and training processes that we think are likely to result in authentic reasoners also provides a kind of defense in depth, alongside the tests of authentic reasoning.\n\nI\u2019ll also note that it\u2019s important that the authentic reasoner models we create be reasonably competitive with those developed by mainstream capabilities, so that they can constitute a safe alternative.\n\nThe most straightforward way we might make a model more dependent on externalized reasoning is to give it fewer parameters, possibly by giving it fewer layers of depth. This way, its capacity to use serial compute internally is reduced, and it may be more dependent on the pseudo-serial compute provided by its externalized reasoning. If there are concerns that a sophisticated model could anticipate and thereby evade our reasoning authenticity tests, it\u2019s possible that using a shallower model could assuage that fear. Ideally, the pseudo-serial depth provided by the chain of thought could make up for the hit to performance that would come from reducing the model\u2019s depth, but if that isn\u2019t the case then this approach alone wouldn\u2019t be capabilities competitive.\n\nAnother idea comes from a modification of the approach in \u201cCensoring Representations with an Adversary\u201d[7], where the adversary would use the model\u2019s internal states to try to derive the answer to the question while the model itself is still in the middle of reasoning. Optimizing the model against the adversary\u2019s success should reduce the extent to which the final answer is represented in the model\u2019s internal states, making it more dependent upon the externalized reasoning. For many reasons this is far from a complete solution, including concerns about the distribution that the model is trained on.\n\nI\u2019ve hardly explored this direction at all; the strategies above are only meant to illustrate that it doesn\u2019t seem impossible a priori to develop architectures or training processes that would likely create authentic reasoners. I anticipate that further thought should quickly yield better strategies than those listed.\n\nThird condition: oversee the reasoning\nConditional on the first two goals being accomplished (assessing reasoning authenticity and creating authentic reasoners), there are at least two techniques that could be used to reduce the chance of deploying unaligned AI: filtering a model\u2019s outputs in deployment based on the acceptability of its reasoning, and fine-tuning the model to reason acceptably.\n\nFor a model in deployment, we could use automated oversight tools that scan its reasoning process for elements we don\u2019t approve of, like reasoning that relates to convergent instrumental subgoals, deception, or that is non-myopic. These tools could be simpler than the system they\u2019re overseeing, and be run in real time against the model\u2019s reasoning before any actions are taken based on the output. If we didn\u2019t approve of the reasoning, we could prevent the action from being taken and raise an alarm instead.\n\nIt\u2019s possible that our default training strategy for models would often result in reasoning that we didn\u2019t approve of. In this case, we could fine-tune the model using RLHF on human approval of the reasoning process itself, and in this way train the model to reason in ways that we approve of. This second approach might start to look something like optimizing against our interpretability technique, which could push the model to obfuscate its reasoning. If we take this path we should be especially wary of steganographic messages being encoded in the reasoning.\n\n \n\nAdvantages of this approach\nTo me, the major advantage of this research direction comes from the fact that success would enable us to express approval or disapproval over the model\u2019s actual reasoning process.\n\nI think this direction is particularly exciting right now for other reasons as well:\n\nExternalized reasoning can be studied in current models, even relatively small ones.\nTests of reasoning authenticity may only require inference access to a model, which is often easier to get than weights and activations.\nIf a model fails these tests, it may constitute a \u201cspooky demo\u201d that\u2019s legible to the wider machine learning research community.\nModels for which externalized reasoning is essential may be smaller and cheaper to train than cutting edge large language models.\nThis research direction also suggests a number of projects that can be worked on right now, in parallel with one another. Some of these include:\n\nDeveloping and implementing better reasoning authenticity tests.\nTraining models that constitute adversarial examples for authenticity tests, like models that encode steganographic messages in their reasoning.\nDeveloping and testing strategies to train authentic reasoners.\nDeveloping tools to automate the oversight of externalized reasoning.\nDe-confusing concepts related to authenticity, reasoning approval, and oversight.\n \n\nShortcomings of this approach\nHere are some reasons that this research direction might fail:\n\nThe first or second conditions may be impossible to achieve.\nWe might not be able to even define what we mean by \u201cauthenticity\u201d to our own satisfaction.\nEven with a sound definition, we might not be able to develop empirical tests to detect it.\nEven with a solid definition and good tests, we might not be able to train models that pass the tests.\nThe third condition may be impossible to achieve\nIt might not be possible to oversee the model\u2019s reasoning process to our satisfaction.\nIt may not be possible to do so with tools that are simple enough for us to trust.\nReinforcement learning on the acceptability of the reasoning process may cause the reasoning to no longer be authentic.\nOther problems may arise\nWe may fool ourselves into thinking that we\u2019ve developed sound definitions, tests, and oversight tools, but there are actually failure modes that we\u2019ve missed which are exploited by the model.\nIt might not be possible to find a level of reasoning granularity that both allows the model to reason well and enables reasoning oversight.\nThe externalized reasoning approach may not work well outside of logical reasoning problems.\nSystems developed this way may not be sufficiently competitive with mainline capabilities to constitute a safe alternative.\n \n\nFinal words\nThis research direction is still just a sketch with many details left to be filled in; I look forward to seeing it developed further. I invite anyone who is interested in it or in similar directions to collaborate with one another and with me.\n\n\n        \n        \n        OVERVIEW STRUCTURE: I. Introduction     A. Background     B. Research Direction II. Assessing Reasoning Authenticity     A. Causal Responsibility     B. Completeness     C. Straightforwardness III. Developing Tests     A. Builder/Breaker methodology IV. Creating Authentic Reasoners     A. Model architectures and training processes     B. Making model more dependent on externalized reasoning     C. Censoring Representations with an Adversary V. Overseeing Reasoning     A. Automated oversight tools     B. Scanning reasoning process for elements not approved of     C. Preventing action from being taken if reasoning not approved ofVI. Advantages of this approach     A. Ability to express approval or disapproval over the model\u2019s actual reasoning process     B. Externalized reasoning can be studied in current models     C. Tests of reasoning authenticity may only require inference access to a model     D. Models for which externalized reasoning is essential may be smaller and cheaper to train     E. Suggests a number of projects that can be worked on right now VII. Shortcomings of this approach     A. Conditions may be impossible to achieve     B. May not be able to define what is meant by \u201cauthenticity\u201d     C. May not be able to develop empirical tests to detect it     D. May not be able to train models that pass the tests     E. May not be able to oversee the model\u2019s reasoning process     F. Reinforcement learning on the acceptability of the reasoning process may cause the reasoning to no longer be authentic     G. Other problems may arise VIII. Final Words     A. This research direction is still just a sketch     B. Invitation to collaborate with one another and with meIX. Conclusion\n\n\n        SUMMARY:", "parentIds": [], "childrenIds": ["2218475101091140941", "3301281408277659022", "2065066273807413945", "-9053103668943442393"], "lastVisited": "1676889647", "group": "path"}, {"id": "2218475101091140941", "text": " This research paper suggests exploring a novel research direction- aligning pretrained language models by testing their authenticity. To do this, it proposes three conditions: assessing the authenticity of the reasoning process, creating authentic reasoners and overseeing the reasoning process. To assess the authenticity of the reasoning process, the paper proposes tests that use causal responsibility, completeness, and straightforwardness. These tests may require inference", "parentIds": ["-9213561883320502024"], "childrenIds": [], "lastVisited": "1676889647"}, {"id": "3301281408277659022", "text": " This paper presents a research direction that focuses on the alignment of pretrained language models with externalized reasoning. The goal is to provide oversight of models by making them think out loud and monitoring the reasoning process. To interpret the authenticity of the reasoning process, tests have already been developed that analyse the causal responsibility, completeness, and straightforwardness of the model's reasoning. The Builder/Breaker methodology", "parentIds": ["-9213561883320502024"], "childrenIds": ["3186512337299666573", "407278562905941059", "42408129844536170", "-5501542145835630270"], "lastVisited": "1676889647", "group": "path"}, {"id": "2065066273807413945", "text": " In this paper, a research direction is proposed to create reliable and safe AI systems. This research direction involves prompting the model to work through logical reasoning problems, developing tests to assess the authenticity of a model's reasoning and overseeing the reasoning process. The tests should be foolproof and must assess the model's causal responsibility, completeness and straightforwardness. To create authentic reasoners, model architectures and training processes", "parentIds": ["-9213561883320502024"], "childrenIds": [], "lastVisited": "1676889647"}, {"id": "-9053103668943442393", "text": " This paper presents a research direction that could be used to align pretrained language models and make sure that their reasoning is authentic. The direction is based on the idea of forcing a model to think out loud, which could then be monitored and supervised. To assess the authenticity of the model\u2019s reasoning, three conditions must be met: causal responsibility, completeness, and straightforwardness. The paper further", "parentIds": ["-9213561883320502024"], "childrenIds": [], "lastVisited": "1676889647"}, {"id": "3186512337299666573", "text": " proposed in the ELK report provides a framework for these tests. To create authentic reasoners, model architectures and training processes have been proposed that make models more dependent on externalized reasoning. Automated oversight tools have also been suggested to scan the reasoning process for elements that are not approved of. This approach has several advantages, such as the ability to express approval or disapproval over the model\u2019s actual", "parentIds": ["3301281408277659022"], "childrenIds": [], "lastVisited": "1676889647"}, {"id": "407278562905941059", "text": " proposed by the ELK report can be used to create more comprehensive tests. Additionally, architectures and training processes can be designed to create models that pass the tests and be competitive with mainstream capabilities. These authentic reasoners can then be deployed with automated oversight tools which scan the reasoning process for elements that are not approved of, preventing action from being taken if the reasoning is not approved of. If successful, this", "parentIds": ["3301281408277659022"], "childrenIds": [], "lastVisited": "1676889647"}, {"id": "42408129844536170", "text": " , which was presented in the ELK report, can be used to develop more complete tests. Additionally, different strategies have been proposed to create models that pass these tests. Examples include reducing the model's parameters and optimizing it against an adversary\u2019s success. Automated oversight tools have also been designed to scan the reasoning process for elements not approved of and prevent action being taken if the reasoning is not", "parentIds": ["3301281408277659022"], "childrenIds": [], "lastVisited": "1676889647"}, {"id": "-5501542145835630270", "text": " can be used to develop such tests and authentic reasoner models can be produced by reducing the model's internal parameters and modifying the \"Censoring Representations with an Adversary\" paper. Automated oversight tools can also be used to filter the model's outputs and fine-tune the reasoning process. The advantages of this approach include the ability to express approval or disapproval over the model\u2019", "parentIds": ["3301281408277659022"], "childrenIds": ["407317436808351541", "2952516851553510176", "-769287765612827251", "6862108480387615363"], "lastVisited": "1676889647", "group": "path"}, {"id": "407317436808351541", "text": " s actual reasoning process, externalized reasoning can be studied in current models, tests of reasoning authenticity may only require inference access to a model and the development of smaller and cheaper models for externalized reasoning. However, the research direction also contains potential pitfalls, such as the impossibility of achieving certain conditions and the potential of reinforcement learning on the acceptability of reasoning process to cause it to become inauthentic.", "parentIds": ["-5501542145835630270"], "childrenIds": [], "lastVisited": "1676889647"}, {"id": "2952516851553510176", "text": " s actual reasoning process, the potential for smaller and cheaper models, and the development of projects that can be worked on right now. However, conditions may be impossible to achieve, and reinforcement learning on the acceptability of the reasoning process may cause the reasoning to no longer be authentic. This research direction is still just a sketch, but an invitation to collaborate with one another and with the author is extended.", "parentIds": ["-5501542145835630270"], "childrenIds": [], "lastVisited": "1676889647", "group": "path"}, {"id": "-769287765612827251", "text": " s actual reasoning process, externalized reasoning can be studied in current models, tests of reasoning authenticity may only require inference access to a model, and models for which externalized reasoning is essential may be smaller and cheaper to train. However, some potential drawbacks include impossibility to achieve the conditions necessary to oversee the reasoning process, lack of the ability to define what is meant by \u201cauthenticity\u201d,", "parentIds": ["-5501542145835630270"], "childrenIds": [], "lastVisited": "1676889647"}, {"id": "6862108480387615363", "text": " s actual reasoning process, the potential to study externalized reasoning in current models, and the possibility of only requiring inference access for tests of reasoning authenticity. However, there are potential shortcomings such as the possibility of conditions being impossible to achieve, defining what is meant by authenticity, and developing empirical tests to detect it. This research direction is still just a sketch and the author invites collaboration with him and with other", "parentIds": ["-5501542145835630270"], "childrenIds": [], "lastVisited": "1676889647"}], "edges": [{"from": "-9213561883320502024", "to": "2218475101091140941", "relation": "parentId"}, {"from": "-9213561883320502024", "to": "3301281408277659022", "relation": "parentId"}, {"from": "-9213561883320502024", "to": "2065066273807413945", "relation": "parentId"}, {"from": "-9213561883320502024", "to": "-9053103668943442393", "relation": "parentId"}, {"from": "3301281408277659022", "to": "3186512337299666573", "relation": "parentId"}, {"from": "3301281408277659022", "to": "407278562905941059", "relation": "parentId"}, {"from": "3301281408277659022", "to": "42408129844536170", "relation": "parentId"}, {"from": "3301281408277659022", "to": "-5501542145835630270", "relation": "parentId"}, {"from": "-5501542145835630270", "to": "407317436808351541", "relation": "parentId"}, {"from": "-5501542145835630270", "to": "2952516851553510176", "relation": "parentId"}, {"from": "-5501542145835630270", "to": "-769287765612827251", "relation": "parentId"}, {"from": "-5501542145835630270", "to": "6862108480387615363", "relation": "parentId"}], "name": "test", "pathNodes": ["-9213561883320502024", "3301281408277659022", "-5501542145835630270", "2952516851553510176"], "focusedId": "2952516851553510176"}