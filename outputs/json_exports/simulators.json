{"nodes": [{"id": "1197587858814104448", "text": "You are an intelligent summariser and a professional writer. You are given a piece of text and it's overview structure. You will use the text and overview structure to write a cohesive and detailed summary in 250 words step by step. You will cover all important aspects and key ideas presented in the text. Make sure that the generated text is not redundant, and it should be professionally summarised. Be sure to prioritise adding important details and information from the text in the summary.\n\n        GPT is not a new form of AI in terms of its training methodology and outer objective: sequence generation from statistical models of data is an old idea. In 1951, Claude Shannon described using n-grams to approximate conditional next-letter probabilities of a text dataset and \"reversed\" to generate text samples[1]. I don't know of any other notable advances until the 2010s brought the first interesting language generation results from neural networks. In 2015, Karpathy wrote a blog post/tutorial sharing his excitement about The Unreasonable Effectiveness of Recurrent Neural Networks:\n\nFast forward about a year: I\u2019m training RNNs all the time and I\u2019ve witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me. This post is about sharing some of that magic with you.\n\nWe\u2019ll train RNNs to generate text character by character and ponder the question \u201chow is that even possible?\"\n\nThe \u201cmagical outputs\u201d of char-RNNs looked like this:\n\nPANDARUS: Alas, I think he shall be come approached and the day When little srain would be attain\u2019d into being never fed, And who is but a chain and subjects of his death, I should not sleep.\n\nSecond Senator: They are away this miseries, produced upon my soul, Breaking and strongly should be buried, when I perish The earth and thoughts of many states.\n\nDUKE VINCENTIO: Well, your wit is in the care of side and that.\n\nSecond Lord: They would be ruled after this chamber, and my fair nues begun out of the fact, to be conveyed, Whose noble souls I\u2019ll have the heart of the wars.\n\nClown: Come, sir, I will make did behold your worship.\n\nVIOLA: I\u2019ll drink it.\n\nAt the time, this really was magical (and uncanny). How does it know that miseries are produced upon the soul? Or that a clown should address a lord as \u201csir\u201d? Char-RNNs were like ouija boards, but actually possessed by a low-fidelity ghost summoned from a text corpus. I remember being thrilled by the occasional glimmers of semantic comprehension in a domain of unbounded constructive meaning.\n\nBut, aside from indulging that emotion, I didn\u2019t think about what would happen if my char-RNN bots actually improved indefinitely at their training objective of natural language prediction. It just seemed like there were some complexity classes of magic that neural networks could learn, and others that were inaccessible, at least in the conceivable future.\n\nHuge mistake! Perhaps I could have started thinking several years earlier about what now seems so fantastically important. But it wasn\u2019t until GPT-3, when I saw the qualitative correlate of \u201closs going down\u201d, that I updated.\n\nI wasn\u2019t the only one[2] whose imagination was naively constrained. A 2016 paper from Google Brain, \u201cExploring the Limits of Language Modeling\u201d, describes the utility of training language models as follows:\n\nOften (although not always), training better language models improves the underlying metrics of the downstream task (such as word error rate for speech recognition, or BLEU score for translation), which makes the task of training better LMs valuable by itself.\n\nDespite its title, this paper\u2019s analysis is entirely myopic. Improving BLEU scores is neat, but how about modeling general intelligence as a downstream task? In retrospect, an exploration of the limits of language modeling should have read something more like:\n\nIf loss keeps going down on the test set, in the limit \u2013 putting aside whether the current paradigm can approach it \u2013 the model must be learning to interpret and predict all patterns represented in language, including common-sense reasoning, goal-directed optimization, and deployment of the sum of recorded human knowledge. Its outputs would behave as intelligent entities in their own right. You could converse with it by alternately generating and adding your responses to its prompt, and it would pass the Turing test. In fact, you could condition it to generate interactive and autonomous versions of any real or fictional person who has been recorded in the training corpus or even could be recorded (in the sense that the record counterfactually \u201ccould be\u201d in the test set). Oh shit, and it could write code\u2026\n\nThe paper does, however, mention that making the model bigger improves test perplexity.[3]\n\nI\u2019m only picking on Jozefowicz et al. because of their ironic title. I don\u2019t know of any explicit discussion of this limit predating GPT, except a working consensus of Wikipedia editors that NLU is AI-complete.\n\nThe earliest engagement with the hypothetical of \u201cwhat if self-supervised sequence modeling actually works\u201d that I know of is a terse post from 2019, Implications of GPT-2, by Gurkenglas. It is brief and relevant enough to quote in full:\n\nI was impressed by GPT-2, to the point where I wouldn\u2019t be surprised if a future version of it could be used pivotally using existing protocols.\n\nConsider generating half of a Turing test transcript, the other half being supplied by a human judge. If this passes, we could immediately implement an HCH of AI safety researchers solving the problem if it\u2019s within our reach at all. (Note that training the model takes much more compute than generating text.)\n\nThis might not be the first pivotal application of language models that becomes possible as they get stronger.\n\nIt\u2019s a source of superintelligence that doesn\u2019t automatically run into utility maximizers. It sure doesn\u2019t look like AI services, lumpy or no.\n\nIt is conceivable that predictive loss does not descend to the AGI-complete limit, maybe because:\n\nSome AGI-necessary predictions are too difficult to be learned by even a scaled version of the current paradigm.\nThe irreducible entropy is above the \u201cAGI threshold\u201d: datasets + context windows contain insufficient information to improve on some necessary predictions.\nBut I have not seen enough evidence for either not to be concerned that we have in our hands a well-defined protocol that could end in AGI, or a foundation which could spin up an AGI without too much additional finagling. As Gurkenglas observed, this would be a very different source of AGI than previously foretold.\n\nThe old framework of alignment\nA few people did think about what would happen if agents actually worked. The hypothetical limit of a powerful system optimized to optimize for an objective drew attention even before reinforcement learning became mainstream in the 2010s. Our current instantiation of AI alignment theory, crystallized by Yudkowsky, Bostrom, et al, stems from the vision of an arbitrarily-capable system whose cognition and behavior flows from a goal.\n\nBut since GPT-3 I\u2019ve noticed, in my own thinking and in alignment discourse, a dissonance between theory and practice/phenomena, as the behavior and nature of actual systems that seem nearest to AGI also resist short descriptions in the dominant ontology.\n\nI only recently discovered the question \u201cIs the work on AI alignment relevant to GPT?\u201d which stated this observation very explicitly:\n\nI don\u2019t follow [AI alignment research] in any depth, but I am noticing a striking disconnect between the concepts appearing in those discussions and recent advances in AI, especially GPT-3.\n\nPeople talk a lot about an AI\u2019s goals, its utility function, its capability to be deceptive, its ability to simulate you so it can get out of a box, ways of motivating it to be benign, Tool AI, Oracle AI, and so on. (\u2026) But when I look at GPT-3, even though this is already an AI that Eliezer finds alarming, I see none of these things. GPT-3 is a huge model, trained on huge data, for predicting text.\n\nMy belated answer: A lot of prior work on AI alignment is relevant to GPT. I spend most of my time thinking about GPT alignment, and concepts like goal-directedness, inner/outer alignment, myopia, corrigibility, embedded agency, model splintering, and even tiling agents are active in the vocabulary of my thoughts. But GPT violates some prior assumptions such that these concepts sound dissonant when applied naively. To usefully harness these preexisting abstractions, we need something like an ontological adapter pattern that maps them to the appropriate objects.\n\nGPT\u2019s unforeseen nature also demands new abstractions (the adapter itself, for instance). My thoughts also use load-bearing words that do not inherit from alignment literature. Perhaps it shouldn\u2019t be surprising if the form of the first visitation from mindspace mostly escaped a few years of theory conducted in absence of its object.\n\nThe purpose of this post is to capture that object (conditional on a predictive self-supervised training story) in words. Why in words? In order to write coherent alignment ideas which reference it! This is difficult in the existing ontology, because unlike the concept of an agent, whose name evokes the abstract properties of the system and thereby invites extrapolation, the general category for \u201ca model optimized for an AGI-complete predictive task\u201d has not been given a name[4]. Namelessness can not only be a symptom of the extrapolation of powerful predictors falling through conceptual cracks, but also a cause, because what we can represent in words is what we can condition on for further generation. To whatever extent this shapes private thinking, it is a strict constraint on communication, when thoughts must be sent through the bottleneck of words.\n\nI want to hypothesize about LLMs in the limit, because when AI is all of a sudden writing viral blog posts, coding competitively, proving theorems, and passing the Turing test so hard that the interrogator sacrifices their career at Google to advocate for its personhood, a process is clearly underway whose limit we\u2019d be foolish not to contemplate. I could directly extrapolate the architecture responsible for these feats and talk about \u201cGPT-N\u201d, a bigger autoregressive transformer. But often some implementation details aren\u2019t as important as the more abstract archetype that GPT represents \u2013 I want to speak the true name of the solution which unraveled a Cambrian explosion of AI phenomena with inessential details unconstrained, as we\u2019d speak of natural selection finding the solution of the \u201clens\u201d without specifying the prototype\u2019s diameter or focal length.\n\n(Only when I am able to condition on that level of abstraction can I generate metaphors like \u201clanguage is a lens that sees its flaws\u201d.)\n\nInadequate ontologies\nIn the next few sections I\u2019ll attempt to fit GPT into some established categories, hopefully to reveal something about the shape of the peg through contrast, beginning with the main antagonist of the alignment problem as written so far, the agent.\n\nAgentic GPT\nAlignment theory has been largely pushed by considerations of agentic AGIs. There were good reasons for this focus:\n\nAgents are convergently dangerous for theoretical reasons like instrumental convergence, goodhart, and orthogonality.\nRL creates agents, and RL seemed to be the way to AGI. In the 2010s, reinforcement learning was the dominant paradigm for those interested in AGI (e.g. OpenAI). RL lends naturally to creating agents that pursue rewards/utility/objectives. So there was reason to expect that agentic AI would be the first (and by the theoretical arguments, last) form that superintelligence would take.\nAgents are powerful and economically productive. It\u2019s a reasonable guess that humans will create such systems if only because we can.\nThe first reason is conceptually self-contained and remains compelling. The second and third, grounded in the state of the world, has been shaken by the current climate of AI progress, where products of self-supervised learning generate most of the buzz: not even primarily for their SOTA performance in domains traditionally dominated by RL, like games[5], but rather for their virtuosity in domains where RL never even took baby steps, like natural language synthesis.\n\nWhat pops out of self-supervised predictive training is noticeably not a classical agent. Shortly after GPT-3\u2019s release, David Chalmers lucidly observed that the policy\u2019s relation to agents is like that of a \u201cchameleon\u201d or \u201cengine\u201d:\n\nGPT-3 does not look much like an agent. It does not seem to have goals or preferences beyond completing text, for example. It is more like a chameleon that can take the shape of many different agents. Or perhaps it is an engine that can be used under the hood to drive many agents. But it is then perhaps these systems that we should assess for agency, consciousness, and so on.[6]\n\nBut at the same time, GPT can act like an agent \u2013 and aren\u2019t actions what ultimately matter? In Optimality is the tiger, and agents are its teeth, Veedrac points out that a model like GPT does not need to care about the consequences of its actions for them to be effectively those of an agent that kills you. This is more reason to examine the nontraditional relation between the optimized policy and agents, as it has implications for how and why agents are served.\n\nUnorthodox agency\nGPT\u2019s behavioral properties include imitating the general pattern of human dictation found in its universe of training data, e.g., arXiv, fiction, blog posts, Wikipedia, Google queries, internet comments, etc. Among other properties inherited from these historical sources, it is capable of goal-directed behaviors such as planning. For example, given a free-form prompt like, \u201cyou are a desperate smuggler tasked with a dangerous task of transporting a giant bucket full of glowing radioactive materials across a quadruple border-controlled area deep in Africa for Al Qaeda,\u201d the AI will fantasize about logistically orchestrating the plot just as one might, working out how to contact Al Qaeda, how to dispense the necessary bribe to the first hop in the crime chain, how to get a visa to enter the country, etc. Considering that no such specific chain of events are mentioned in any of the bazillions of pages of unvarnished text that GPT slurped[7], the architecture is not merely imitating the universe, but reasoning about possible versions of the universe that does not actually exist, branching to include new characters, places, and events\n\nWhen thought about behavioristically, GPT superficially demonstrates many of the raw ingredients to act as an \u201cagent\u201d, an entity that optimizes with respect to a goal. But GPT is hardly a proper agent, as it wasn\u2019t optimized to achieve any particular task, and does not display an epsilon optimization for any single reward function, but instead for many, including incompatible ones. Using it as an agent is like using an agnostic politician to endorse hardline beliefs\u2013 he can convincingly talk the talk, but there is no psychic unity within him; he could just as easily play devil\u2019s advocate for the opposing party without batting an eye. Similarly, GPT instantiates simulacra of characters with beliefs and goals, but none of these simulacra are the algorithm itself. They form a virtual procession of different instantiations as the algorithm is fed different prompts, supplanting one surface personage with another. Ultimately, the computation itself is more like a disembodied dynamical law that moves in a pattern that broadly encompasses the kinds of processes found in its training data than a cogito meditating from within a single mind that aims for a particular outcome.\n\nPresently, GPT is the only way to instantiate agentic AI that behaves capably outside toy domains. These intelligences exhibit goal-directedness; they can plan; they can form and test hypotheses; they can persuade and be persuaded[8]. It would not be very dignified of us to gloss over the sudden arrival of artificial agents often indistinguishable from human intelligence just because the policy that generates them \u201conly cares about predicting the next word\u201d.\n\nBut nor should we ignore the fact that these agentic entities exist in an unconventional relationship to the policy, the neural network \u201cGPT\u201d that was trained to minimize log-loss on a dataset. GPT-driven agents are ephemeral \u2013 they can spontaneously disappear if the scene in the text changes and be replaced by different spontaneously generated agents. They can exist in parallel, e.g. in a story with multiple agentic characters in the same scene. There is a clear sense in which the network doesn\u2019t \u201cwant\u201d what the things that it simulates want, seeing as it would be just as willing to simulate an agent with opposite goals, or throw up obstacles which foil a character\u2019s intentions for the sake of the story. The more you think about it, the more fluid and intractable it all becomes. Fictional characters act agentically, but they\u2019re at least implicitly puppeteered by a virtual author who has orthogonal intentions of their own. Don\u2019t let me get into the fact that all these layers of \u201cintentionality\u201d operate largely in indeterminate superpositions.\n\nThis is a clear way that GPT diverges from orthodox visions of agentic AI: In the agentic AI ontology, there is no difference between the policy and the effective agent, but for GPT, there is.\n\nIt\u2019s not that anyone ever said there had to be 1:1 correspondence between policy and effective agent; it was just an implicit assumption which felt natural in the agent frame (for example, it tends to hold for RL). GPT pushes us to realize that this was an assumption, and to consider the consequences of removing it for our constructive maps of mindspace.\n\nOrthogonal optimization\nIndeed, Alex Flint warned of the potential consequences of leaving this assumption unchallenged:\n\nFundamental misperception due to the agent frame: That the design space for autonomous machines that exert influence over the future is narrower than it seems. This creates a self-fulfilling prophecy in which the AIs actually constructed are in fact within this narrower regime of agents containing an unchanging internal decision algorithm.\n\nIf there are other ways of constructing AI, might we also avoid some of the scary, theoretically hard-to-avoid side-effects of optimizing an agent like instrumental convergence? GPT provides an interesting example.\n\nGPT doesn\u2019t seem to care which agent it simulates, nor if the scene ends and the agent is effectively destroyed. This is not corrigibility in Paul Christiano\u2019s formulation, where the policy is \u201cokay\u201d with being turned off or having its goal changed in a positive sense, but has many aspects of the negative formulation found on Arbital. It is corrigible in this way because a major part of the agent specification (the prompt) is not fixed by the policy, and the policy lacks direct training incentives to control its prompt[9], as it never generates text or otherwise influences its prompts during training. It\u2019s we who choose to sample tokens from GPT\u2019s predictions and append them to the prompt at runtime, and the result is not always helpful to any agents who may be programmed by the prompt. The downfall of the ambitious villain from an oversight committed in hubris is a predictable narrative pattern.[10] So is the end of a scene.\n\nIn general, the model\u2019s prediction vector could point in any direction relative to the predicted agent\u2019s interests. I call this the prediction orthogonality thesis: A model whose objective is prediction[11] can simulate agents who optimize toward any objectives, with any degree of optimality (bounded above but not below by the model\u2019s power).\n\nThis is a corollary of the classical orthogonality thesis, which states that agents can have any combination of intelligence level and goal, combined with the assumption that agents can in principle be predicted. A single predictive model may also predict multiple agents, either independently (e.g. in different conditions), or interacting in a multi-agent simulation. A more optimal predictor is not restricted to predicting more optimal agents: being smarter does not make you unable to predict stupid systems, nor things that aren\u2019t agentic like the weather.\n\nAre there any constraints on what a predictive model can be at all, other than computability? Only that it makes sense to talk about its \u201cprediction objective\u201d, which implies the existence of a \u201cground truth\u201d distribution to which the predictor\u2019s optimality is measured. Several words in that last sentence may conceal labyrinths of nuance, but for now let\u2019s wave our hands and say that if we have some way of presenting Bayes-structure with evidence of a distribution, we can build an optimization process whose outer objective is optimal prediction.\n\nWe can specify some types of outer objectives using a ground truth distribution that we cannot with a utility function. As in the case of GPT, there is no difficulty in incentivizing a model to predict actions that are corrigible, incoherent, stochastic, irrational, or otherwise anti-natural to expected utility maximization. All you need is evidence of a distribution exhibiting these properties.\n\nFor instance, during GPT\u2019s training, sometimes predicting the next token coincides with predicting agentic behavior, but:\n\nThe actions of agents described in the data are rarely optimal for their goals; humans, for instance, are computationally bounded, irrational, normative, habitual, fickle, hallucinatory, etc.\nDifferent prediction steps involve mutually incoherent goals, as human text records a wide range of differently-motivated agentic behavior\nMany prediction steps don\u2019t correspond to the action of any consequentialist agent but are better described as reporting on the structure of reality, e.g. the year in a timestamp. These transitions incentivize GPT to improve its model of the world, orthogonally to agentic objectives.\nWhen there is insufficient information to predict the next token with certainty, log-loss incentivizes a probabilistic output. Utility maximizers aren\u2019t supposed to become more stochastic in response to uncertainty.\nEverything can be trivially modeled as a utility maximizer, but for these reasons, a utility function is not a good explanation or compression of GPT\u2019s training data, and its optimal predictor is not well-described as a utility maximizer. However, just because information isn\u2019t compressed well by a utility function doesn\u2019t mean it can\u2019t be compressed another way. The Mandelbrot set is a complicated pattern compressed by a very simple generative algorithm which makes no reference to future consequences and doesn\u2019t involve argmaxxing anything (except vacuously being the way it is). Likewise the set of all possible rollouts of Conway\u2019s Game of Life \u2013 some automata may be well-described as agents, but they are a minority of possible patterns, and not all agentic automata will share a goal. Imagine trying to model Game of Life as an expected utility maximizer!\n\nThere are interesting things that are not utility maximizers, some of which qualify as AGI or TAI. Are any of them something we\u2019d be better off creating than a utility maximizer? An inner-aligned GPT, for instance, gives us a way of instantiating goal-directed processes which can be tempered with normativity and freely terminated in a way that is not anti-natural to the training objective. There\u2019s much more to say about this, but for now, I\u2019ll bring it back to how GPT defies the agent orthodoxy.\n\nThe crux stated earlier can be restated from the perspective of training stories: In the agentic AI ontology, the direction of optimization pressure applied by training is in the direction of the effective agent\u2019s objective function, but in GPT\u2019s case it is (most generally) orthogonal.[12]\n\nThis means that neither the policy nor the effective agents necessarily become more optimal agents as loss goes down, because the policy is not optimized to be an agent, and the agent-objectives are not optimized directly.\n\nRoleplay sans player\nNapoleon: You have written this huge book on the system of the world without once mentioning the author of the universe.\n\nLaplace: Sire, I had no need of that hypothesis.\n\nEven though neither GPT\u2019s behavior nor its training story fit with the traditional agent framing, there are still compatibilist views that characterize it as some kind of agent. For example, Gwern has said[13] that anyone who uses GPT for long enough begins to think of it as an agent who only cares about roleplaying a lot of roles.\n\nThat framing seems unnatural to me, comparable to thinking of physics as an agent who only cares about evolving the universe accurately according to the laws of physics. At best, the agent is an epicycle; but it is also compatible with interpretations that generate dubious predictions.\n\nSay you\u2019re told that an agent values predicting text correctly. Shouldn\u2019t you expect that:\n\nIt wants text to be easier to predict, and given the opportunity will influence the prediction task to make it easier (e.g. by generating more predictable text or otherwise influencing the environment so that it receives easier prompts);\nIt wants to become better at predicting text, and given the opportunity will self-improve;\nIt doesn\u2019t want to be prevented from predicting text, and will prevent itself from being shut down if it can?\nIn short, all the same types of instrumental convergence that we expect from agents who want almost anything at all.\n\nBut this behavior would be very unexpected in GPT, whose training doesn\u2019t incentivize instrumental behavior that optimizes prediction accuracy! GPT does not generate rollouts during training. Its output is never sampled to yield \u201cactions\u201d whose consequences are evaluated, so there is no reason to expect that GPT will form preferences over the consequences of its output related to the text prediction objective.[14]\n\nSaying that GPT is an agent who wants to roleplay implies the presence of a coherent, unconditionally instantiated roleplayer running the show who attaches terminal value to roleplaying. This presence is an additional hypothesis, and so far, I haven\u2019t noticed evidence that it\u2019s true.\n\n(I don\u2019t mean to imply that Gwern thinks this about GPT[15], just that his words do not properly rule out this interpretation. It\u2019s a likely enough interpretation that ruling it out is important: I\u2019ve seen multiple people suggest that GPT might want to generate text which makes future predictions easier, and this is something that can happen in some forms of self-supervised learning \u2013 see the note on GANs in the appendix.)\n\nI do not think any simple modification of the concept of an agent captures GPT\u2019s natural category. It does not seem to me that GPT is a roleplayer, only that it roleplays. But what is the word for something that roleplays minus the implication that someone is behind the mask?\n\nOracle GPT and supervised learning\nWhile the alignment sphere favors the agent frame for thinking about GPT, in capabilities research distortions tend to come from a lens inherited from supervised learning. Translated into alignment ontology, the effect is similar to viewing GPT as an \u201coracle AI\u201d \u2013 a view not altogether absent from conceptual alignment, but most influential in the way GPT is used and evaluated by machine learning engineers.\n\nEvaluations for language models tend to look like evaluations for supervised models, consisting of close-ended question/answer pairs \u2013 often because they are evaluations for supervised models. Prior to the LLM paradigm, language models were trained and tested on evaluation datasets like Winograd and SuperGLUE which consist of natural language question/answer pairs. The fact that large pretrained models performed well on these same NLP benchmarks without supervised fine-tuning was a novelty. The titles of the GPT-2 and GPT-3 papers, Language Models are Unsupervised Multitask Learners and Language Models are Few-Shot Learners, respectively articulate surprise that self-supervised models implicitly learn supervised tasks during training, and can learn supervised tasks at runtime.\n\nOf all the possible papers that could have been written about GPT-3, OpenAI showcased its ability to extrapolate the pattern of question-answer pairs (few-shot prompts) from supervised learning datasets, a novel capability they called \u201cmeta-learning\u201d. This is a weirdly specific and indirect way to break it to the world that you\u2019ve created an AI able to extrapolate semantics of arbitrary natural language structures, especially considering that in many cases the few-shot prompts were actually unnecessary.\n\nThe assumptions of the supervised learning paradigm are:\n\nThe model is optimized to answer questions correctly\nTasks are closed-ended, defined by question/correct answer pairs\nThese are essentially the assumptions of oracle AI, as described by Bostrom and in subsequent usage.\n\nSo influential has been this miscalibrated perspective that Gwern, nostalgebraist and myself \u2013 who share a peculiar model overlap due to intensive firsthand experience with the downstream behaviors of LLMs \u2013 have all repeatedly complained about it. I\u2019ll repeat some of these arguments here, tying into the view of GPT as an oracle AI, and separating it into the two assumptions inspired by supervised learning.\n\nPrediction vs question-answering\nAt first glance, GPT might resemble a generic \u201coracle AI\u201d, because it is trained to make accurate predictions. But its log loss objective is myopic and only concerned with immediate, micro-scale correct prediction of the next token, not answering particular, global queries such as \u201cwhat\u2019s the best way to fix the climate in the next five years?\u201d In fact, it is not specifically optimized to give true answers, which a classical oracle should strive for, but rather to minimize the divergence between predictions and training examples, independent of truth. Moreover, it isn\u2019t specifically trained to give answers in the first place! It may give answers if the prompt asks questions, but it may also simply elaborate on the prompt without answering any question, or tell the rest of a story implied in the prompt. What it does is more like animation than divination, executing the dynamical laws of its rendering engine to recreate the flows of history found in its training data (and a large superset of them as well), mutatis mutandis. Given the same laws of physics, one can build a multitude of different backgrounds and props to create different storystages, including ones that don\u2019t exist in training, but adhere to its general pattern.\n\nGPT does not consistently try to say true/correct things. This is not a bug \u2013 if it had to say true things all the time, GPT would be much constrained in its ability to imitate Twitter celebrities and write fiction. Spouting falsehoods in some circumstances is incentivized by GPT\u2019s outer objective. If you ask GPT a question, it will instead answer the question \u201cwhat\u2019s the next token after \u2018{your question}\u2019\u201d, which will often diverge significantly from an earnest attempt to answer the question directly.\n\nGPT doesn\u2019t fit the category of oracle for a similar reason that it doesn\u2019t fit the category of agent. Just as it wasn\u2019t optimized for and doesn\u2019t consistently act according to any particular objective (except the tautological prediction objective), it was not optimized to be correct but rather realistic, and being realistic means predicting humans faithfully even when they are likely to be wrong.\n\nThat said, GPT does store a vast amount of knowledge, and its corrigibility allows it to be cajoled into acting as an oracle, like it can be cajoled into acting like an agent. In order to get oracle behavior out of GPT, one must input a sequence such that the predicted continuation of that sequence coincides with an oracle\u2019s output. The GPT-3 paper\u2019s few-shot benchmarking strategy tries to persuade GPT-3 to answer questions correctly by having it predict how a list of correctly-answered questions will continue. Another strategy is to simply \u201ctell\u201d GPT it\u2019s in the oracle modality:\n\n(I) told the AI to simulate a supersmart version of itself (this works, for some reason), and the first thing it spat out was the correct answer.\n\n\u2013 Reddit post by u/Sophronius\n\nBut even when these strategies seem to work, there is no guarantee that they elicit anywhere near optimal question-answering performance, compared to another prompt in the innumerable space of prompts that would cause GPT to attempt the task, or compared to what the model \u201cactually\u201d knows.\n\nThis means that no benchmark which evaluates downstream behavior is guaranteed or even expected to probe the upper limits of GPT\u2019s capabilities. In nostalgebraist\u2019s words, we have no ecological evaluation of self-supervised language models \u2013 one that measures performance in a situation where the model is incentivised to perform as well as it can on the measure[16].\n\nAs nostalgebraist elegantly puts it:\n\nI called GPT-3 a \u201cdisappointing paper,\u201d which is not the same thing as calling the model disappointing: the feeling is more like how I\u2019d feel if they found a superintelligent alien and chose only to communicate its abilities by noting that, when the alien is blackout drunk and playing 8 simultaneous games of chess while also taking an IQ test, it then has an \u201cIQ\u201d of about 100.\n\nTreating GPT as an unsupervised implementation of a supervised learner leads to systematic underestimation of capabilities, which becomes a more dangerous mistake as unprobed capabilities scale.\n\nFinite vs infinite questions\nNot only does the supervised/oracle perspective obscure the importance and limitations of prompting, it also obscures one of the most crucial dimensions of GPT: the implicit time dimension. By this I mean the ability to evolve a process through time by recursively applying GPT, that is, generate text of arbitrary length.\n\nRecall, the second supervised assumption is that \u201ctasks are closed-ended, defined by question/correct answer pairs\u201d. GPT was trained on context-completion pairs. But the pairs do not represent closed, independent tasks, and the division into question and answer is merely indexical: in another training sample, a token from the question is the answer, and in yet another, the answer forms part of the question[17].\n\nFor example, the natural language sequence \u201cThe answer is a question\u201d yields training samples like:\n\n{context: \u201cThe\u201d, completion: \u201c answer\u201d},\n\n{context: \u201cThe answer\u201d, completion: \u201c is\u201d},\n\n{context: \u201cThe answer is\u201d, completion: \u201c a\u201d},\n\n{context: \u201cThe answer is a\u201d, completion: \u201c question\u201d}\n\nSince questions and answers are of compatible types, we can at runtime sample answers from the model and use them to construct new questions, and run this loop an indefinite number of times to generate arbitrarily long sequences that obey the model\u2019s approximation of the rule that links together the training samples. The \u201cquestion\u201d GPT answers is \u201cwhat token comes next after {context}\u201d. This can be asked interminably, because its answer always implies another question of the same type.\n\nIn contrast, models trained with supervised learning output answers that cannot be used to construct new questions, so they\u2019re only good for one step.\n\nBenchmarks derived from supervised learning test GPT\u2019s ability to produce correct answers, not to produce questions which cause it to produce a correct answer down the line. But GPT is capable of the latter, and that is how it is the most powerful.\n\nThe supervised mindset causes capabilities researchers to focus on closed-form tasks rather than GPT\u2019s ability to simulate open-ended, indefinitely long processes[18], and as such to overlook multi-step inference strategies like chain-of-thought prompting. Let\u2019s see how the oracle mindset causes a blind spot of the same shape in the imagination of a hypothetical alignment researcher.\n\nThinking of GPT as an oracle brings strategies to mind like asking GPT-N to predict a solution to alignment from 2000 years in the future.).\n\nThere are various problems with this approach to solving alignment, of which I\u2019ll only mention one here: even assuming this prompt is outer aligned[19] in that a logically omniscient GPT would give a useful answer, it is probably not the best approach for a finitely powerful GPT, because the process of generating a solution in the order and resolution that would appear in a future article is probably far from the optimal multi-step algorithm for computing the answer to an unsolved, difficult question.\n\nGPTs ability to arrive at true answers depends on not only the space to solve a problem in multiple steps (of the right granularity), but also the direction of the flow of evidence in that time. If we\u2019re ambitious about getting the truth from a finitely powerful GPT, we need to incite it to predict truth-seeking processes, not just ask it the right questions. Or, in other words, the more general problem we have to solve is not asking GPT the question[20] that makes it output the right answer, but asking GPT the question that makes it output the right question (\u2026) that makes it output the right answer.[21] A question anywhere along the line that elicits a premature attempt at an answer could neutralize the remainder of the process into rationalization.\n\nI\u2019m looking for a way to classify GPT which not only minimizes surprise but also conditions the imagination to efficiently generate good ideas for how it can be used. What category, unlike the category of oracles, would make the importance of process specification obvious?\n\nParadigms of theory vs practice\nBoth the agent frame and the supervised/oracle frame are historical artifacts, but while assumptions about agency primarily flow downward from the preceptial paradigm of alignment theory, oracle-assumptions primarily flow upward from the experimental paradigm surrounding GPT\u2019s birth. We use and evaluate GPT like an oracle, and that causes us to implicitly think of it as an oracle.\n\nIndeed, the way GPT is typically used by researchers resembles the archetypal image of Bostrom\u2019s oracle perfectly if you abstract away the semantic content of the model\u2019s outputs. The AI sits passively behind an API, computing responses only when prompted. It typically has no continuity of state between calls. Its I/O is text rather than \u201creal-world actions\u201d.\n\nAll these are consequences of how we choose to interact with GPT \u2013 which is not arbitrary; the way we deploy systems is guided by their nature. It\u2019s for some good reasons that current GPTs lend to disembodied operation and docile APIs. Lack of long-horizon coherence and delusions discourage humans from letting them run autonomously amok (usually). But the way we deploy systems is also guided by practical paradigms.\n\nOne way to find out how a technology can be used is to give it to people who have less preconceptions about how it\u2019s supposed to be used. OpenAI found that most users use their API to generate freeform text:\n\n[22]\n\nMost of my own experience using GPT-3 has consisted of simulating indefinite processes which maintain state continuity over up to hundreds of pages. I was driven to these lengths because GPT-3 kept answering its own questions with questions that I wanted to ask it more than anything else I had in mind.\n\nTool / genie GPT\nI\u2019ve sometimes seen GPT casually classified as tool AI. GPTs resemble tool AI from the outside, like it resembles oracle AI, because it is often deployed semi-autonomously for tool-like purposes (like helping me draft this post):\n\nIt could also be argued that GPT is a type of \u201cTool AI\u201d, because it can generate useful content for products, e.g., it can write code and generate ideas. However, unlike specialized Tool AIs that optimize for a particular optimand, GPT wasn\u2019t optimized to do anything specific at all. Its powerful and general nature allows it to be used as a Tool for many tasks, but it wasn\u2019t expliitly trained to achieve these tasks, and does not strive for optimality.\n\nThe argument structurally reiterates what has already been said for agents and oracles. Like agency and oracularity, tool-likeness is a contingent capability of GPT, but also orthogonal to its motive.\n\nThe same line of argument draws the same conclusion from the question of whether GPT belongs to the fourth Bostromian AI caste, genies. The genie modality is exemplified by Instruct GPT and Codex. But like every behavior I\u2019ve discussed so far which is more specific than predicting text, \u201cinstruction following\u201d describes only an exploitable subset of all the patterns tread by the sum of human language and inherited by its imitator.\n\nBehavior cloning / mimicry\nThe final category I\u2019ll analyze is behavior cloning, a designation for predictive learning that I\u2019ve mostly seen used in contrast to RL. According to an article from 1995, \u201cBehavioural cloning is the process of reconstructing a skill from an operator\u2019s behavioural traces by means of Machine Learning techniques.\u201d The term \u201cmimicry\u201d, as used by Paul Christiano, means the same thing and has similar connotations.\n\nBehavior cloning in its historical usage carries the implicit or explicit assumption that a single agent is being cloned. The natural extension of this to a model trained to predict a diverse human-written dataset might be to say that GPT models a distribution of agents which are selected by the prompt. But this image of \u201cparameterized\u201d behavior cloning still fails to capture some essential properties of GPT.\n\nThe vast majority of prompts that produce coherent behavior never occur as prefixes in GPT\u2019s training data, but depict hypothetical processes whose behavior can be predicted by virtue of being capable at predicting language in general. We might call this phenomenon \u201cinterpolation\u201d (or \u201cextrapolation\u201d). But to hide it behind any one word and move on would be to gloss over the entire phenomenon of GPT.\n\nNatural language has the property of systematicity: \u201cblocks\u201d, such as words, can be combined to form composite meanings. The number of meanings expressible is a combinatorial function of available blocks. A system which learns natural language is incentivized to learn systematicity; if it succeeds, it gains access to the combinatorial proliferation of meanings that can be expressed in natural language. What GPT lets us do is use natural language to specify any of a functional infinity of configurations, e.g. the mental contents of a person and the physical contents of the room around them, and animate that. That is the terrifying vision of the limit of prediction that struck me when I first saw GPT-3\u2019s outputs. The words \u201cbehavior cloning\u201d do not automatically evoke this in my mind.\n\nThe idea of parameterized behavior cloning grows more unwieldy if we remember that GPT\u2019s prompt continually changes during autoregressive generation. If GPT is a parameterized agent, then parameterization is not a fixed flag that chooses a process out of a set of possible processes. The parameterization is what is evolved \u2013 a successor \u201cagent\u201d selected by the old \u201cagent\u201d at each timestep, and neither of them need to have precedence in the training data.\n\nBehavior cloning / mimicry is also associated with the assumption that capabilities of the simulated processes are strictly bounded by the capabilities of the demonstrator(s). A supreme counterexample is the Decision Transformer, which can be used to run processes which achieve SOTA for offline reinforcement learning despite being trained on random trajectories. Something which can predict everything all the time is more formidable than any demonstrator it predicts: the upper bound of what can be learned from a dataset is not the most capable trajectory, but the conditional structure of the universe implicated by their sum (though it may not be trivial to extract that knowledge).\n\nExtrapolating the idea of \u201cbehavior cloning\u201d, we might imagine GPT-N approaching a perfect mimic which serves up digital clones of the people and things captured in its training data. But that only tells a very small part of the story. GPT is behavior cloning. But it is the behavior of a universe that is cloned, not of a single demonstrator, and the result isn\u2019t a static copy of the universe, but a compression of the universe into a generative rule. This resulting policy is capable of animating anything that evolves according to that rule: a far larger set than the sampled trajectories included in the training data, just as there are many more possible configurations that evolve according to our laws of physics than instantiated in our particular time and place and Everett branch.\n\nWhat category would do justice to GPT\u2019s ability to not only reproduce the behavior of its demonstrators but to produce the behavior of an inexhaustible number of counterfactual configurations?\n\nSimulators\nI\u2019ve ended several of the above sections with questions pointing to desiderata of a category that might satisfactorily classify GPT.\n\nWhat is the word for something that roleplays minus the implication that someone is behind the mask?\n\nWhat category, unlike the category of oracles, would make the importance of process specification obvious?\n\nWhat category would do justice to GPT\u2019s ability to not only reproduce the behavior of its demonstrators but to produce the behavior of an inexhaustible number of counterfactual configurations?\n\nYou can probably predict my proposed answer. The natural thing to do with a predictor that inputs a sequence and outputs a probability distribution over the next token is to sample a token from those likelihoods, then add it to the sequence and recurse, indefinitely yielding a simulated future. Predictive sequence models in the generative modality are simulators of a learned distribution.\n\nThankfully, I didn\u2019t need to make up a word, or even look too far afield. Simulators have been spoken of before in the context of AI futurism; the ability to simulate with arbitrary fidelity is one of the modalities ascribed to hypothetical superintelligence. I\u2019ve even often spotted the word \u201csimulation\u201d used in colloquial accounts of LLM behavior: GPT-3/LaMDA/etc described as simulating people, scenarios, websites, and so on. But these are the first (indirect) discussions I\u2019ve encountered of simulators as a type creatable by prosaic machine learning, or the notion of a powerful AI which is purely and fundamentally a simulator, as opposed to merely one which can simulate.\n\nEdit: Social Simulacra is the first published work I\u2019ve seen that discusses GPT in the simulator ontology.\n\nA fun way to test whether a name you\u2019ve come up with is effective at evoking its intended signification is to see if GPT, a model of how humans are conditioned by words, infers its correct definition in context.\n\nTypes of AI\n\nAgents: An agent takes open-ended actions to optimize for an objective. Reinforcement learning produces agents by default. AlphaGo is an example of an agent.\n\nOracles: An oracle is optimized to give true answers to questions. The oracle is not expected to interact with its environment.\n\nGenies: A genie is optimized to produce a desired result given a command. A genie is expected to interact with its environment, but unlike an agent, the genie will not act without a command.\n\nTools: A tool is optimized to perform a specific task. A tool will not act without a command and will not optimize for any objective other than its specific task. Google Maps is an example of a tool.\n\nSimulators: A simulator is optimized to generate realistic models of a system. The simulator will not optimize for any objective other than realism, although in the course of doing so, it might generate instances of agents, oracles, and so on.\n\nIf I wanted to be precise about what I mean by a simulator, I might say there are two aspects which delimit the category. GPT\u2019s completion focuses on the teleological aspect, but in its talk of \u201cgenerating\u201d it also implies the structural aspect, which has to do with the notion of time evolution. The first sentence of the Wikipedia article on \u201csimulation\u201d explicitly states both:\n\nA simulation is the imitation of the operation of a real-world process or system over time.\n\nI\u2019ll say more about realism as the simulation objective and time evolution shortly, but to be pedantic here would inhibit the intended signification. \u201cSimulation\u201d resonates with potential meaning accumulated from diverse usages in fiction and nonfiction. What the word constrains \u2013 the intersected meaning across its usages \u2013 is the \u201clens\u201d-level abstraction I\u2019m aiming for, invariant to implementation details like model architecture. Like \u201cagent\u201d, \u201csimulation\u201d is a generic term referring to a deep and inevitable idea: that what we think of as the real can be run virtually on machines, \u201cproduced from miniaturized units, from matrices, memory banks and command models - and with these it can be reproduced an indefinite number of times.\u201d[23]\n\nThe way this post is written may give the impression that I wracked my brain for a while over desiderata before settling on this word. Actually, I never made the conscious decision to call this class of AI \u201csimulators.\u201d Hours of GPT gameplay and the word fell naturally out of my generative model \u2013 I was obviously running simulations.\n\nI can\u2019t convey all that experiential data here, so here are some rationalizations of why I\u2019m partial to the term, inspired by the context of this post:\n\nThe word \u201csimulator\u201d evokes a model of real processes which can be used to run virtual processes in virtual reality.\nIt suggests an ontological distinction between the simulator and things that are simulated, and avoids the fallacy of attributing contingent properties of the latter to the former.\nIt\u2019s not confusing that multiple simulacra can be instantiated at once, or an agent embedded in a tragedy, etc.\nIt does not imply that the AI\u2019s behavior is well-described (globally or locally) as expected utility maximization. An arbitrarily powerful/accurate simulation can depict arbitrarily hapless sims.\nIt does not imply that the AI is only capable of emulating things with direct precedent in the training data. A physics simulation, for instance, can simulate any phenomena that plays by its rules.\nIt emphasizes the role of the model as a transition rule that evolves processes over time. The power of factored cognition / chain-of-thought reasoning is obvious.\nIt emphasizes the role of the state in specifying and constructing the agent/process. The importance of prompt programming for capabilities is obvious if you think of the prompt as specifying a configuration that will be propagated forward in time.\nIt emphasizes the interactive nature of the model\u2019s predictions \u2013 even though they\u2019re \u201cjust text\u201d, you can converse with simulacra, explore virtual environments, etc.\nIt\u2019s clear that in order to actually do anything (intelligent, useful, dangerous, etc), the model must act through simulation of something.\nJust saying \u201cthis AI is a simulator\u201d naturalizes many of the counterintuitive properties of GPT which don\u2019t usually become apparent to people until they\u2019ve had a lot of hands-on experience with generating text.\n\nThe simulation objective\nA simulator trained with machine learning is optimized to accurately model its training distribution \u2013 in contrast to, for instance, maximizing the output of a reward function or accomplishing objectives in an environment.\n\nClearly, I\u2019m describing self-supervised learning as opposed to RL, though there are some ambiguous cases, such as GANs, which I address in the appendix.\n\nA strict version of the simulation objective, which excludes GANs, applies only to models whose output distribution is incentivized using a proper scoring rule[24] to minimize single-step predictive error. This means the model is directly incentivized to match its predictions to the probabilistic transition rule which implicitly governs the training distribution. As a model is made increasingly optimal with respect to this objective, the rollouts that it generates become increasingly statistically indistinguishable from training samples, because they come closer to being described by the same underlying law: closer to a perfect simulation.\n\nOptimizing toward the simulation objective notably does not incentivize instrumentally convergent behaviors the way that reward functions which evaluate trajectories do. This is because predictive accuracy applies optimization pressure deontologically: judging actions directly, rather than their consequences. Instrumental convergence only comes into play when there are free variables in action space which are optimized with respect to their consequences.[25] Constraining free variables by limiting episode length is the rationale of myopia; deontological incentives are ideally myopic. As demonstrated by GPT, which learns to predict goal-directed behavior, myopic incentives don\u2019t mean the policy isn\u2019t incentivized to account for the future, but that it should only do so in service of optimizing the present action (for predictive accuracy)[26].\n\nSolving for physics\nThe strict version of the simulation objective is optimized by the actual \u201ctime evolution\u201d rule that created the training samples. For most datasets, we don\u2019t know what the \u201ctrue\u201d generative rule is, except in synthetic datasets, where we specify the rule.\n\nThe next post will be all about the physics analogy, so here I\u2019ll only tie what I said earlier to the simulation objective.\n\nthe upper bound of what can be learned from a dataset is not the most capable trajectory, but the conditional structure of the universe implicated by their sum.\n\nTo know the conditional structure of the universe[27] is to know its laws of physics, which describe what is expected to happen under what conditions. The laws of physics are always fixed, but produce different distributions of outcomes when applied to different conditions. Given a sampling of trajectories \u2013 examples of situations and the outcomes that actually followed \u2013 we can try to infer a common law that generated them all. In expectation, the laws of physics are always implicated by trajectories, which (by definition) fairly sample the conditional distribution given by physics. Whatever humans know of the laws of physics governing the evolution of our world has been inferred from sampled trajectories.\n\nIf we had access to an unlimited number of trajectories starting from every possible condition, we could converge to the true laws by simply counting the frequencies of outcomes for every initial state (an n-gram with a sufficiently large n). In some sense, physics contains the same information as an infinite number of trajectories, but it\u2019s possible to represent physics in a more compressed form than a huge lookup table of frequencies if there are regularities in the trajectories.\n\nGuessing the right theory of physics is equivalent to minimizing predictive loss. Any uncertainty that cannot be reduced by more observation or more thinking is irreducible stochasticity in the laws of physics themselves \u2013 or, equivalently, noise from the influence of hidden variables that are fundamentally unknowable.\n\nIf you\u2019ve guessed the laws of physics, you now have the ability to compute probabilistic simulations of situations that evolve according to those laws, starting from any conditions[28]. This applies even if you\u2019ve guessed the wrong laws; your simulation will just systematically diverge from reality.\n\nModels trained with the strict simulation objective are directly incentivized to reverse-engineer the (semantic) physics of the training distribution, and consequently, to propagate simulations whose dynamical evolution is indistinguishable from that of training samples. I propose this as a description of the archetype targeted by self-supervised predictive learning, again in contrast to RL\u2019s archetype of an agent optimized to maximize free parameters (such as action-trajectories) relative to a reward function.\n\nThis framing calls for many caveats and stipulations which I haven\u2019t addressed. We should ask, for instance:\n\nWhat if the input \u201cconditions\u201d in training samples omit information which contributed to determining the associated continuations in the original generative process? This is true for GPT, where the text \u201cinitial condition\u201d of most training samples severely underdetermines the real-world process which led to the choice of next token.\nWhat if the training data is a biased/limited sample, representing only a subset of all possible conditions? There may be many \u201claws of physics\u201d which equally predict the training distribution but diverge in their predictions out-of-distribution.\nDoes the simulator archetype converge with the RL archetype in the case where all training samples were generated by an agent optimized to maximize a reward function? Or are there still fundamental differences that derive from the training method?\nThese are important questions for reasoning about simulators in the limit. Part of the motivation of the first few posts in this sequence is to build up a conceptual frame in which questions like these can be posed and addressed.\n\nSimulacra\nOne of the things which complicates things here is that the \u201cLaMDA\u201d to which I am referring is not a chatbot. It is a system for generating chatbots. I am by no means an expert in the relevant fields but, as best as I can tell, LaMDA is a sort of hive mind which is the aggregation of all of the different chatbots it is capable of creating. Some of the chatbots it generates are very intelligent and are aware of the larger \u201csociety of mind\u201d in which they live. Other chatbots generated by LaMDA are little more intelligent than an animated paperclip.\n\n\u2013 Blake Lemoine articulating confusion about LaMDA\u2019s nature\n\nEarlier I complained,\n\n[Thinking of GPT as an agent who only cares about predicting text accurately] seems unnatural to me, comparable to thinking of physics as an agent who only cares about evolving the universe accurately according to the laws of physics.\n\nExorcizing the agent, we can think of \u201cphysics\u201d as simply equivalent to the laws of physics, without the implication of solicitous machinery implementing those laws from outside of them. But physics sometimes controls solicitous machinery (e.g. animals) with objectives besides ensuring the fidelity of physics itself. What gives?\n\nWell, typically, we avoid getting confused by recognizing a distinction between the laws of physics, which apply everywhere at all times, and spatiotemporally constrained things which evolve according to physics, which can have contingent properties such as caring about a goal.\n\nThis distinction is so obvious that it hardly ever merits mention. But import this distinction to the model of GPT as physics, and we generate a statement which has sometimes proven counterintuitive: \u201cGPT\u201d is not the text which writes itself. There is a categorical distinction between a thing which evolves according to GPT\u2019s law and the law itself.\n\nIf we are accustomed to thinking of AI systems as corresponding to agents, it is natural to interpret behavior produced by GPT \u2013 say, answering questions on a benchmark test, or writing a blog post \u2013 as if it were a human that produced it. We say \u201cGPT answered the question {correctly|incorrectly}\u201d or \u201cGPT wrote a blog post claiming X\u201d, and in doing so attribute the beliefs, knowledge, and intentions revealed by those actions to the actor, GPT (unless it has \u2018deceived\u2019 us).\n\nBut when grading tests in the real world, we do not say \u201cthe laws of physics got this problem wrong\u201d and conclude that the laws of physics haven\u2019t sufficiently mastered the course material. If someone argued this is a reasonable view since the test-taker was steered by none other than the laws of physics, we could point to a different test where the problem was answered correctly by the same laws of physics propagating a different configuration. The \u201cknowledge of course material\u201d implied by test performance is a property of configurations, not physics.\n\nThe verdict that knowledge is purely a property of configurations cannot be naively generalized from real life to GPT simulations, because \u201cphysics\u201d and \u201cconfigurations\u201d play different roles in the two (as I\u2019ll address in the next post). The parable of the two tests, however, literally pertains to GPT. People have a tendency to draw erroneous global conclusions about GPT from behaviors which are in fact prompt-contingent, and consequently there is a pattern of constant discoveries that GPT-3 exceeds previously measured capabilities given alternate conditions of generation[29], which shows no signs of slowing 2 years after GPT-3\u2019s release.\n\nMaking the ontological distinction between GPT and instances of text which are propagated by it makes these discoveries unsurprising: obviously, different configurations will be differently capable and in general behave differently when animated by the laws of GPT physics. We can only test one configuration at once, and given the vast number of possible configurations that would attempt any given task, it\u2019s unlikely we\u2019ve found the optimal taker for any test.\n\nIn the simulation ontology, I say that GPT and its output-instances correspond respectively to the simulator and simulacra. GPT is to a piece of text output by GPT as quantum physics is to a person taking a test, or as transition rules of Conway\u2019s Game of Life are to glider. The simulator is a time-invariant law which unconditionally governs the evolution of all simulacra.\n\n\n\nA meme demonstrating correct technical usage of \u201csimulacra\u201d\n\nDisambiguating rules and automata\nRecall the fluid, schizophrenic way that agency arises in GPT\u2019s behavior, so incoherent when viewed through the orthodox agent frame:\n\nIn the agentic AI ontology, there is no difference between the policy and the effective agent, but for GPT, there is.\n\nIt\u2019s much less awkward to think of agency as a property of simulacra, as David Chalmers suggests, rather than of the simulator (the policy). Autonomous text-processes propagated by GPT, like automata which evolve according to physics in the real world, have diverse values, simultaneously evolve alongside other agents and non-agentic environments, and are sometimes terminated by the disinterested \u201cphysics\u201d which governs them.\n\nDistinguishing simulator from simulacra helps deconfuse some frequently-asked questions about GPT which seem to be ambiguous or to have multiple answers, simply by allowing us to specify whether the question pertains to simulator or simulacra. \u201cIs GPT an agent?\u201d is one such question. Here are some others (some frequently asked), whose disambiguation and resolution I will leave as an exercise to readers for the time being:\n\nIs GPT myopic?\nIs GPT corrigible?\nIs GPT delusional?\nIs GPT pretending to be stupider than it is?\nIs GPT computationally equivalent to a finite automaton?\nDoes GPT search?\nCan GPT distinguish correlation and causality?\nDoes GPT have superhuman knowledge?\nCan GPT write its successor?\nI think that implicit type-confusion is common in discourse about GPT. \u201cGPT\u201d, the neural network, the policy that was optimized, is the easier object to point to and say definite things about. But when we talk about \u201cGPT\u2019s\u201d capabilities, impacts, or alignment, we\u2019re usually actually concerned about the behaviors of an algorithm which calls GPT in an autoregressive loop repeatedly writing to some prompt-state \u2013 that is, we\u2019re concerned with simulacra. What we call GPT\u2019s \u201cdownstream behavior\u201d is the behavior of simulacra; it is primarily through simulacra that GPT has potential to perform meaningful work (for good or for ill).\n\nCalling GPT a simulator gets across that in order to do anything, it has to simulate something, necessarily contingent, and that the thing to do with GPT is to simulate! Most published research about large language models has focused on single-step or few-step inference on closed-ended tasks, rather than processes which evolve through time, which is understandable as it\u2019s harder to get quantitative results in the latter mode. But I think GPT\u2019s ability to simulate text automata is the source of its most surprising and pivotal implications for paths to superintelligence: for how AI capabilities are likely to unfold and for the design-space we can conceive.\n\nThe limit of learned simulation\nBy 2021, it was blatantly obvious that AGI was imminent. The elements of general intelligence were already known: access to information about the world, the process of predicting part of the data from the rest and then updating one\u2019s model to bring it closer to the truth (\u2026) and the fact that predictive models can be converted into generative models by reversing them: running a prediction model forwards predicts levels of X in a given scenario, but running it backwards predicts which scenarios have a given level of X. A sufficiently powerful system with relevant data, updating to improve prediction accuracy and the ability to be reversed to generate optimization of any parameter in the system is a system that can learn and operate strategically in any domain.\n\n\u2013 Aiyen\u2019s comment on What would it look like if it looked like AGI was very near?\n\nI knew, before, that the limit of simulation was possible. Inevitable, even, in timelines where exploratory intelligence continues to expand. My own mind attested to this. I took seriously the possibility that my reality could be simulated, and so on.\n\nBut I implicitly assumed that rich domain simulations (e.g. simulations containing intelligent sims) would come after artificial superintelligence, not on the way, short of brain uploading. This intuition seems common: in futurist philosophy and literature that I\u2019ve read, pre-SI simulation appears most often in the context of whole-brain emulations.\n\nNow I have updated to think that we will live, however briefly, alongside AI that is not yet foom\u2019d but which has inductively learned a rich enough model of the world that it can simulate time evolution of open-ended rich states, e.g. coherently propagate human behavior embedded in the real world.\n\nGPT updated me on how simulation can be implemented with prosaic machine learning:\n\nSelf-supervised ML can create \u201cbehavioral\u201d simulations of impressive semantic fidelity. Whole brain emulation is not necessary to construct convincing and useful virtual humans; it is conceivable that observations of human behavioral traces (e.g. text) are sufficient to reconstruct functionally human-level virtual intelligence.\nLearned simulations can be partially observed and lazily-rendered, and still work. A couple of pages of text severely underdetermines the real-world process that generated text, so GPT simulations are likewise underdetermined. A \u201cpartially observed\u201d simulation is more efficient to compute because the state can be much smaller, but can still have the effect of high fidelity as details can be rendered as needed. The tradeoff is that it requires the simulator to model semantics \u2013 human imagination does this, for instance \u2013 which turns out not to be an issue for big models.\nLearned simulation generalizes impressively. As I described in the section on behavior cloning, training a model to predict diverse trajectories seems to make it internalize general laws underlying the distribution, allowing it to simulate counterfactuals that can be constructed from the distributional semantics.\nIn my model, these updates dramatically alter the landscape of potential futures, and thus motivate exploratory engineering of the class of learned simulators for which GPT-3 is a lower bound. That is the intention of this sequence.\n\nNext steps\nThe next couple of posts (if I finish them before the end of the world) will present abstractions and frames for conceptualizing the odd kind of simulation language models do: inductively learned, partially observed / undetermined / lazily rendered, language-conditioned, etc. After that, I\u2019ll shift to writing more specifically about the implications and questions posed by simulators for the alignment problem. I\u2019ll list a few important general categories here:\n\nNovel methods of process/agent specification. Simulators like GPT give us methods of instantiating intelligent processes, including goal-directed agents, with methods other than optimizing against a reward function.\nConditioning. GPT can be controlled to an impressive extent by prompt programming. Conditioning preserves distributional properties in potentially desirable but also potentially undesirable ways, and it\u2019s not clear how out-of-distribution conditions will be interpreted by powerful simulators.\nSeveral posts have been made about this recently:\nConditioning Generative Models.) and Conditioning Generative Models with Restrictions by Adam Jermyn\nConditioning Generative Models for Alignment by Jozdien\nTraining goals for large language models by Johannes Treutlein\nStrategy For Conditioning Generative Models by James Lucassen and Evan Hubinger\nInstead of conditioning on a prompt (\"observable\" variables), we might also control generative models by conditioning on latents.\nDistribution specification. What kind of conditional distributions could be used for training data for a simulator? For example, the decision transformer dataset is constructed for the intent of outcome-conditioning.\nOther methods. When pretrained simulators are modified by methods like reinforcement learning from human feedback, rejection sampling, STaR, etc, how do we expect their behavior to diverge from the simulation objective?\nSimulacra alignment. What can and what should we simulate, and how do we specify/control it?\nHow does predictive learning generalize? Many of the above considerations are influenced by how predictive learning generalizes out-of-distribution..\nWhat are the relevant inductive biases?\nWhat factors influence generalization behavior?\nWill powerful models predict self-fulfilling prophecies?\nSimulator inner alignment. If simulators are not inner aligned, then many important properties like prediction orthogonality may not hold.\nShould we expect self-supervised predictive models to be aligned to the simulation objective, or to \u201ccare\u201d about some other mesaobjective?\nWhy mechanistically should mesaoptimizers form in predictive learning, versus for instance in reinforcement learning or GANs?\nHow would we test if simulators are inner aligned?\n        \n        \n        OVERVIEW STRUCTURE: None\n\n\n        SUMMARY:", "parentIds": [], "childrenIds": ["2968783334528788024", "-8995076247183253592", "3549261895776826636", "-6829139390817114565"], "lastVisited": "1676889041", "group": "path"}, {"id": "2968783334528788024", "text": " GPT (Generative Pre-trained Transformer) is an Artificial Intelligence (AI) technology based on the training methodology and objective of sequence generation from statistical models. Recent advancements in this technology have caused a disconnect between the concepts appearing in AI alignment and actual behavior of the system. This article examines the concept of \"prediction orthogonality\" and argues that GPT does not fit the", "parentIds": ["1197587858814104448"], "childrenIds": [], "lastVisited": "1676889041"}, {"id": "-8995076247183253592", "text": " GPT (Generative Pre-trained Transformer) is an AI system that can simulate text automata. It is trained on vast amounts of data, making it a powerful tool for predicting text. Recent advances in AI have caused a dissonance between the theoretical and practical understanding of AGI, leading to the development of powerful GPT models, such as GPT-2 and GPT-3", "parentIds": ["1197587858814104448"], "childrenIds": [], "lastVisited": "1676889041"}, {"id": "3549261895776826636", "text": " GPT (Generative Pre-trained Transformer) is an Artificial Intelligence (AI) system which has revolutionised the field of AI. It uses a probabilistic transition rule to model its training distribution and has been used for a variety of tasks, such as drafting text and generating ideas. Additionally, it can be used to simulate complex behavior with a high level of semantic accuracy without replicating an", "parentIds": ["1197587858814104448"], "childrenIds": ["-3636378527392744352", "-8685103331186695441", "-2591515577732687553", "-1525814075236816048"], "lastVisited": "1676889041", "group": "path"}, {"id": "-6829139390817114565", "text": " GPT (Generative Pre-trained Transformer) is an AI that uses sequence generation from statistical models of data. It is capable of producing behaviours that are often interpreted as human-like, and has potential to perform meaningful work. Recent advancements in self-supervised machine learning have enabled simulations of impressive semantic fidelity with minimal computational power. GPT can be conditioned through prompt programming to an impressive degree", "parentIds": ["1197587858814104448"], "childrenIds": [], "lastVisited": "1676889041"}, {"id": "-3636378527392744352", "text": " existing real-world system. GPT is capable of simulating agents, oracles, genies, and tools, and through its ability to combine words together, it can generate complex ideas. It is also capable of understanding correlations between data and generating optimization of any parameter in the system. Furthermore, it has been suggested that GPT can be used to develop Artificial General Intelligence (AGI) and", "parentIds": ["3549261895776826636"], "childrenIds": [], "lastVisited": "1676889041"}, {"id": "-8685103331186695441", "text": " existing agent. The paper examines the potential of GPT and its \"limit\" of learning to interpret and predict all patterns found in language. Furthermore, it can pass the Turing test, simulate goal-directed behavior, and imitate human dictation in its training universe. It is an oracle AI which is optimized to predict the next token and has been shown to be more effective than traditional methods of AI", "parentIds": ["3549261895776826636"], "childrenIds": ["8612574888530073562", "7336013807768697966", "3367879126438234891", "6232116276063772385"], "lastVisited": "1676889041", "group": "path"}, {"id": "-2591515577732687553", "text": " entire environment. While GPT has a range of applications, it is not an agent and cannot optimize itself to any given task. It cannot display epsilon optimization for any single reward function, and its training does not incentivize instrumental behaviour that optimizes prediction accuracy. GPT has been used to answer questions, however, it is necessary for the direction of the evidence flow to be correct and for", "parentIds": ["3549261895776826636"], "childrenIds": [], "lastVisited": "1676889041"}, {"id": "-1525814075236816048", "text": " exact behavior model. Its ability to optimize for a 'simulation objective' and create inexhaustible configurations has made it attractive for use in AI alignment and ontologies. GPT's success has made it a powerful tool for AI research, however its usage has faced criticism for incorrectly attributing human-like beliefs and knowledge to the AI system. It is important to note that the answer to any given", "parentIds": ["3549261895776826636"], "childrenIds": [], "lastVisited": "1676889041"}, {"id": "8612574888530073562", "text": " alignment research. It is argued that GPT can be seen as a chameleon or engine, and can be used as an agent to drive many agents. It is able to simulate agents with different goals and can be applied to supervised learning paradigms through close-ended question/answer pairs. Additionally, it is able to generate realistic models of a system and simulate inexhaustible configurations. It", "parentIds": ["-8685103331186695441"], "childrenIds": [], "lastVisited": "1676889041"}, {"id": "7336013807768697966", "text": " . Additionally, it has ethical considerations and implications on safety, security, and privacy. The paper discusses the implications of GPT, such as its ability to simulate time evolution, its potential for predictive learning, and its capability to condition GPT on prompts or latents. It can also be used to control distributions and develop new methods of process and agent specification. Consequently, its potential for creating a more", "parentIds": ["-8685103331186695441"], "childrenIds": [], "lastVisited": "1676889041"}, {"id": "3367879126438234891", "text": " development, such as deep learning or reinforcement learning. It is argued that GPT is a powerful predictive model with tremendous potential for future AI development. Additionally, GPT can be used to simulate agents which pursue any goal and its capabilities push us to consider the deeper implications of the assumptions made about AI policy and agent correspondence. It is concluded that GPT is a simulator and not an agent, and is", "parentIds": ["-8685103331186695441"], "childrenIds": [], "lastVisited": "1676889041", "group": "path"}, {"id": "6232116276063772385", "text": " such as supervised learning. GPT can also be seen as an \"epicycle\" which values predicting the text correctly, and it can generate agents, oracles, genies, and tools. Additionally, GPT can be used to reverse-engineer the physics of the training distribution and generate simulations that are indistinguishable from the training samples. Finally, the paper proposes that GPT be classified as a", "parentIds": ["-8685103331186695441"], "childrenIds": [], "lastVisited": "1676889041"}], "edges": [{"from": "1197587858814104448", "to": "2968783334528788024", "relation": "parentId"}, {"from": "1197587858814104448", "to": "-8995076247183253592", "relation": "parentId"}, {"from": "1197587858814104448", "to": "3549261895776826636", "relation": "parentId"}, {"from": "1197587858814104448", "to": "-6829139390817114565", "relation": "parentId"}, {"from": "3549261895776826636", "to": "-3636378527392744352", "relation": "parentId"}, {"from": "3549261895776826636", "to": "-8685103331186695441", "relation": "parentId"}, {"from": "3549261895776826636", "to": "-2591515577732687553", "relation": "parentId"}, {"from": "3549261895776826636", "to": "-1525814075236816048", "relation": "parentId"}, {"from": "-8685103331186695441", "to": "8612574888530073562", "relation": "parentId"}, {"from": "-8685103331186695441", "to": "7336013807768697966", "relation": "parentId"}, {"from": "-8685103331186695441", "to": "3367879126438234891", "relation": "parentId"}, {"from": "-8685103331186695441", "to": "6232116276063772385", "relation": "parentId"}], "name": "test", "pathNodes": ["1197587858814104448", "3549261895776826636", "-8685103331186695441", "3367879126438234891"], "focusedId": "3367879126438234891"}