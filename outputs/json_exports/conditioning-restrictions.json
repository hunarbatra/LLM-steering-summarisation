{"nodes": [{"id": "4147638508526822386", "text": "You are an intelligent summariser and a professional writer. You are given a piece of text and it's overview structure. You will use the text and overview structure to write a cohesive and detailed summary in 250 words step by step. You will cover all important aspects and key ideas presented in the text. Make sure that the generated text is not redundant, and it should be professionally summarised. Be sure to prioritise adding important details and information from the text in the summary.\n\n        Conditioning Generative Models with Restrictions\n\nBackground\nPreviously, I wrote about ways we could use a generative language model to produce alignment research. There I proposed two approaches:\n\nSimulate a superhuman alignment researcher by conditioning on them generating superhuman machine-verifiable results in a variety of related domains. Try to align this superhuman agent by crafting scenarios where a misaligned agent has an incentive to defect (\"Honeypots\").\nSimulate humans performing alignment research for a long time, by conditioning on observations spaced many years apart (\"Patient Research\").\nI think that both approaches are an improvement over doing nothing, but I suspect that no amount of honeypot construction actually ensures an aligned agent, and so it seems that simulating humans (\"Patient Research\") is the more promising direction.\n\nOverview\nIf we truly have a perfect generative model I think the \u201cPatient Research\u201d approach really does well. We can set up a world where machine learning practice is strictly banned and where researchers spend thousands of years constructing a machine-verifiable scheme for AI alignment. The ban prevents that world from containing malign superintelligences, and the time-scale means that if alignment is something humans can solve, the model should produce that solution.\n\nThe problem is that we won\u2019t have perfect generative models, so we\u2019ll have to accept some limitations.\n\nIn this post I\u2019ll explore what changes if we cannot condition on worlds too different from our own. The idea is to understand what happens if our generative model has trouble generalizing too far away from its training data.\n\nFailure Modes\nThe main dangerous failure mode with using generative models to produce alignment research is that we accidentally ask for a future that contains a deceptive AGI.\n\nIf we get such a future, the AGI might spoof observations to trick us into importing it into our world. For instance it could pretend to be a human producing really good alignment research, but produce work which is subtly flawed and, once implemented, allows it to take over the world.\n\nThere are generally two ways we hit this failure mode.\n\nThe first is that we ask for a prediction of the distant future. The future 50 years from now has a decent chance of containing a deceptive AGI, and if we naively ask for that future there\u2019s a good chance we\u2019ll simulate that AGI, be deceived, and import that AGI into our world.\n\nThe second way is to ask for a world that is very unlikely in ways that make deceptive AGI more likely. For instance, we could condition on observing that next year someone produces what appears to be a perfect solution to AI alignment. When we do this, the hypothesis \u201ca deceptive AGI took over and pretended to solve alignment\u201d becomes a lot more likely, and so the generative model is more likely to simulate that scenario.\n\nThe name of the game, then, is to craft strategies that simultaneously (1) make deceptive AGI\u2019s less likely and (2) accelerate alignment research.\n\nGround Rules\nObservations\nWe have a generative model trained on observations of the world. The training objective is to predict later observations given earlier ones. We assume that the model is inner-aligned, so that it really does try to predict the future conditioned on observations.\n\nObservations are multi-modal, and can be anything measurable about the world (e.g. images, video, audio, text, contents of an SD card, etc.). Predicted observations do not need to be made by actual sensors, but do need to be ones that could in principle be made were the right sensors in the right places.\n\nConditioning\nIt is possible that our model could know how to construct arbitrary trajectories through time, and so might be able to produce good counterfactual histories. A worry though is that it won\u2019t generalize fully, and in particular that it will over-index on observations in the training set.\n\nFor instance, it may be that counterfactual conditionals that should avert the industrial revolution nevertheless result in trajectories that happen to lead to industrialization so that airplanes continue to appear in the observations later on, because airplane featured prominently in the training set. If this happens, the model is producing samples from an artificially narrow (and improbable!) distribution, which could cause all manner of unlikely and/or unpredictable outputs.\n\nTo avoid this issue we assume that the model is able to generalize to futures consistent with the observations in its training set, but not to counterfactual pasts. As a result, all conditioning has to be made with respect to future observations. Specifically, observations must come after the end of the training set. This is a much stricter requirement than the one in Conditioning Generative Models, and gives us much less room to craft counterfactual realities.\n\nIn principle we can make some counterfactual historical trajectories by only training the model on data up to some past date. The tradeoff is that the model will require enormous amounts of data to train, and pushing that date back to e.g. before the Internet probably makes the training set too small to be useful.\n\nNo Fixed Points\nWe don\u2019t want the model to try to solve for fixed points in its output. This sort of thing arises most naturally when we try to use the model recursively (e.g. suggesting scenarios where human researchers use the model to simulate human researchers using the model, etc.). In such cases the model may try to produce self-fulfilling prophecies, resulting in optimization pressure to steer the future in particular (likely simple) ways.\n\nThis seems very dangerous, so I\u2019d like to rule out all scenario that could result in simulated queries to simulated copies of the model. Unfortunately this is not as easy as just not conditioning on the model\u2019s output.\n\nTo see why, note that we are right now contemplating querying this model in the future. It seems very likely that simulated humans will want to query the model too. We can try to condition on human researchers never querying generative models, but that\u2019s a very unlikely future. An unfortunate consequence of this is that the model may think that a likely reason the humans don\u2019t query the model is because there is a deceptive AGI manipulating the world (see \u201cFailure Modes\u201d above), and so this conditional raises the odds that we see outputs from worlds with deceptive AGI\u2019s, which is bad.\n\nI see two ways out of this problem:\n\nPush the end of the training set back far enough that the model itself doesn\u2019t appear in the training data, and that no one would plausibly build the model during the simulated time-span. That restricts our questions to near-past counterfactuals (see \u201cConditioning\u201d above), but does seem like a robust solution.\nCondition on a very early disaster that prevents the model from being used (e.g. global GPU shortage, policy change banning generative models, etc.). This one pushes down the chance that the model gets used but also pushes down the chance of any other advanced AI existing, which makes it much better than just conditioning on the model not being used. This may still net increase the odds of simulating a world with deceptive AGI, but it\u2019s better than the alternatives.\nNeither of these is totally satisfactory, and I think this general challenge of avoiding self-fulfilling prophecies is an important one to think more about. It seems conceivable that there are just ways to dissolve the problem entirely, which would be great. Broader discussions of this challenge and possible approaches can be found in Training goals for large language models and Conditioning Generative Models for Alignment.\n\nFinally, none of this prevents us from asking the model a question, getting an answer, and then using that answer to craft new questions. The problems I\u2019m worried only apply to recursive (rather than sequential) applications of the model.\n\nFactored Cognition Approaches\nGenerate 5 Years of Alignment Research\nA simple approach is to ask for a copy of the AI Alignment Forums 5 years into the future. This doesn\u2019t require any conditioning, and gives us a five year lead on where we would have been without the generative model. There is a small chance that we can research produced by a misaligned AGI, but that chance can be made very small by tuning the amount of time. If you think that five years is too long, and makes deceptive AGI\u2019s too likely, you can substitute a shorter timeframe into this approach.\n\nDirect 5 Years of Alignment Research\nWe can do better than just generating five years of research though! Instead, we can direct that research. We can condition on researchers spending more of their time on what we believe to be the most important parts of the problem. Mild interventions like this seem unlikely to make the generative model invoke deceptive AGI\u2019s, and could plausibly significantly increase the progress we see.\n\nAsk What to Ask\nEven better than directing research, we can allow research to proceed for five years and then ask the model what humans wish they had done instead. This could be done with a prompt of the form\n\nThe research note with the title \u201cWhat I wish we had worked on 5 years ago\u201d was posted by Paul Christiano to the AI Alignment Forums in 2027. Its contents follow.\n\nNow we might disagree with the arguments in such a post, but this seems like an extremely valuable way to understand the pitfalls of different agendas without actually having to explore them ourselves.\n\nBuild up a Picture\nThe above three approaches, which I\u2019ll refer to as \u201cGenerate\u201d, \u201cDirect\u201d, and \u201cAsk\u201d, can be run in parallel. We can generate many trajectories of alignment research. We can increase the diversity of those trajectories, and answer specific questions, by directing research towards the topics that seem most crucial. And we can solicit feedback from humans in these simulated worlds on what they wish they had done, which we can then incorporate into our judgements of what to ask next.\n\nIncidentally, this approach is a special case of Factored Cognition. ****It is also closely related to HCH, but is importantly not recursive (e.g. the humans we simulate do not themselves consult simulations of themselves).\n\nWhat if we need more time?\nTo the extent that AI Alignment turns out to be a broad subject, by which I mean one that factors nicely into many parallel subproblems, I expect the above approaches to work well.\n\nUnfortunately the problem may turn out to be very deep. It may be that what we need is a great deal of sequential research. That\u2019s much harder to get out of our model.\n\nTo see why, note that the obvious thing to do here is to ask for research conducted over longer time horizons. The more we extend the horizon, though, the more likely it is that we sample from research produced by a deceptive agent. That seems very bad.\n\nAlternatively, we can try to be clever and \u201cimport\u201d research from the future repeatedly. For instance we can first ask our model to produce research from 5 years out. Then, we can condition our model on that research existing today, and again ask it for research 5 years out. The problem with this approach is that conditioning on future research suddenly appearing today almost guarantees that there is a powerful AGI involved, which could well be deceptive, and that again is very bad.\n\nIf we want more time, then, we need to condition the model in ways that make AGI timelines longer.\n\nNo More GPU\u2019s\nProsaic AGI is likely to require ML chips (e.g. GPU\u2019s, TPU\u2019s, etc.). Most of these are designed in the US and manufactured abroad.\n\nSuppose we condition the model on the US banning the export of ML chip designs. The US has a long history of misguided protectionist policies, and ML chips could be called a national security issue, so this sort of conditional should be pretty mild.\n\nThe effect, though, is that ML chip manufacturing is set back by a few years as new fabs are built in the US. This extends the horizon where we can safely assume there are no deceptive AGI\u2019s, which lets us answer deeper questions.\n\nRedirecting Civilization\nWe can plausibly get even more time by a three-step process:\n\nPush AGI off into the future.\nSteer civilization with conditionals such that we don\u2019t want to build AGI.\nConduct thousands of years of alignment research.\nFor example, it seems unlikely but not implausible that a religion emerges that sees dignity only in the fruits of human work. If such a religion were sufficiently prominent it could result in efforts steering away from AI research and towards e.g. human exploration of space.\n\nI think this kind of approach could work, but the order of operations is important! If we start conditioning on lots of unlikely and strongly correlated shifts in human values and society (as in (2)), the most likely story for how this happens is that there\u2019s a deceptive AGI behind the scenes manipulating humanity. To make sure the model doesn\u2019t generate that story, we step (1) needs to come first and make AGI in the relevant time period extremely unlikely.\n\nPulling this off seems tricky, but we can use the generative model to our advantage. We can buy ourselves a few extra years on (1), then explore timelines conditioned in different ways to try to arrange (2). To be careful we could start with very mild conditioning and explore them gradually (e.g. one month at a time), searching for a likely (but entirely human) path away from developing AGI.\n\nSummary\nEven with more restrictions, generative models still look pretty good! In particular, even if we can\u2019t condition on past counterfactuals, it looks like there are plausible ways to safely extract an awful lot of good alignment research by conditioning generative models.\n\nOne theme that comes up again and again in thinking through these scenarios is that there are only really two levers we can pull on. The first is to make human alignment research more productive by guiding it towards more promising paths and by parallelizing across different simulations as much as possible. The second is to make AGI less likely over some timeframe, buying us more time to safely simulate human researchers.\n\nBoth approaches seem important, but which one matters most depends a lot on the (unknown) nature of the problem. If we can solve AI alignment by exploring in great breadth, the first lever is more useful. On the other hand, if the problem requires great depth, meaning sequential thinking, we the second lever is where we should focus.\n        \n        \n        OVERVIEW STRUCTURE: I. Introduction A. Background B. Overview II. Leveraging Generative Models A. Redirecting Civilization B. Leveraging Generative Models III. Ground Rules A. Observations B. Conditioning IV. Conclusion A. Two levers to pull on B. Lever depends on nature of problem\n\n\n        SUMMARY:", "parentIds": [], "childrenIds": ["-1844886676710458658", "4144293079476866784", "-8539761558200713310", "2339257800227025559"], "lastVisited": "1676889734", "group": "path"}, {"id": "-1844886676710458658", "text": " This paper explores the potential of leveraging generative models to redirect civilization in a desirable direction. It begins by providing a background on generative models and their potential for use. It then provides an overview of the challenges and opportunities of leveraging generative models for this purpose. Ground rules for the use of generative models are discussed, such as observations and conditioning. Using generative models to simulate a superhuman alignment", "parentIds": ["4147638508526822386"], "childrenIds": [], "lastVisited": "1676889734"}, {"id": "4144293079476866784", "text": " This paper examines the use of generative models in redirecting civilization to avoid self-fulfilling prophecies. The author introduces two levers on generative models with restrictions to improve alignment research. The first approach is to simulate a superhuman alignment researcher by conditioning on them generating superhuman machine-verifiable results in a variety of related domains. The second approach is to simulate humans performing alignment research for a", "parentIds": ["4147638508526822386"], "childrenIds": ["-8554366541556919582", "2160399242215679546", "-6509981157393593334", "4089777812670243708"], "lastVisited": "1676889734", "group": "path"}, {"id": "-8539761558200713310", "text": " This paper explores various solutions to the AI alignment problem. It begins by discussing the background of the problem and providing an overview of the solutions proposed. The paper then outlines two solutions for leveraging generative models to modify the trajectory of civilization. The first involves conditioning on observations that come after the end of the training set, while the second requires pushing back the end of the training set or directing research towards", "parentIds": ["4147638508526822386"], "childrenIds": [], "lastVisited": "1676889734"}, {"id": "2339257800227025559", "text": " This paper examines the possibility of leveraging generative models for AI Alignment research. It introduces the background and overview of the paper before delving into how generative models can be leveraged to redirect civilization. Three approaches to leverage generative models are discussed: Generate, Direct, and Ask. Ground rules on observations and conditionals are also discussed. The conclusion states that two levers can be pulled depending", "parentIds": ["4147638508526822386"], "childrenIds": [], "lastVisited": "1676889734"}, {"id": "-8554366541556919582", "text": " short period of time and then redirecting civilization in a way that cannot be reused. However, both approaches come with their own drawbacks. The author suggests pushing the end of the training set back far enough so that the model itself doesn\u2019t appear over a long period of time and conditioning on making AGI less likely over some timeframe. Finally, it is noted that prosaic AGI will likely", "parentIds": ["4144293079476866784"], "childrenIds": [], "lastVisited": "1676889734"}, {"id": "2160399242215679546", "text": " set period of time by conditioning on them finding the best path to a variety of future goals. The author suggests this \u201cAsk What to Ask\u201d approach can be used in conjunction with \u201cDirect 5 Years of Alignment Research\u201d and \u201cGenerate 5 Years of Alignment Research\u201d. The author concludes that both of these approaches are important, but the lever that matters most", "parentIds": ["4144293079476866784"], "childrenIds": [], "lastVisited": "1676889734"}, {"id": "-6509981157393593334", "text": " long time and then ask the model what humans wish they had done instead. The paper argues that both levers are important, but the right one to focus on depends on the unknown nature of the problem. If the problem requires great depth, then the second lever is where attention should be focused, while if it can be solved by exploring in great breadth, then the first one is more useful. Finally,", "parentIds": ["4144293079476866784"], "childrenIds": [], "lastVisited": "1676889734"}, {"id": "4089777812670243708", "text": " given number of years and then asking the model what they wish they had done instead. The author suggests that by leveraging generative models, it is possible to redirect civilization in a safe way, buying more time to safely simulate researchers. The paper points out that the most useful lever depends on the nature of the problem, and the author argues that if alignment research requires great depth, then the second lever is", "parentIds": ["4144293079476866784"], "childrenIds": ["1308388098926162502", "2949554739902585927", "3116073812452461853", "-8379321662745430291"], "lastVisited": "1676889734", "group": "path"}, {"id": "1308388098926162502", "text": " more appropriate. Additionally, the author outlines two further strategies to avoid self-fulfilling prophecies and concludes that this challenge requires further research in order to solve it entirely. This paper provides an overview of the potential of generative models for redirecting civilization and offers a way of understanding the different levers available for using generative models for the betterment of our species.", "parentIds": ["4089777812670243708"], "childrenIds": [], "lastVisited": "1676889734"}, {"id": "2949554739902585927", "text": " more suitable. Additionally, the paper cautions that ML chips are mostly manufactured abroad, which can pose additional challenges. Ultimately, the paper argues that by pushing AGI off into the future and conducting thousands of years of alignment research, it is possible to buy extra time to work on AI alignment.", "parentIds": ["4089777812670243708"], "childrenIds": [], "lastVisited": "1676889734"}, {"id": "3116073812452461853", "text": " the more appropriate approach. Additionally, the author suggests that to avoid self-fulfilling prophecies, it is best to push the end of the training set back far enough that the model itself doesn\u2019t appear in the training data, or to condition on a very early disaster that prevents the model from being used. Lastly, the author notes that most ML chips used for AGI are designed", "parentIds": ["4089777812670243708"], "childrenIds": [], "lastVisited": "1676889734", "group": "path"}, {"id": "-8379321662745430291", "text": " more effective. Additionally, the paper outlines two ground rules. The first ground rule is that observations should be multi-modal, and the second is that the model should be inner-aligned. The author also warns against self-fulfilling prophecies and suggests pushing the end of the training set back far enough that the model itself is not in the training data. Furthermore, the author suggests conditioning", "parentIds": ["4089777812670243708"], "childrenIds": [], "lastVisited": "1676889734"}], "edges": [{"from": "4147638508526822386", "to": "-1844886676710458658", "relation": "parentId"}, {"from": "4147638508526822386", "to": "4144293079476866784", "relation": "parentId"}, {"from": "4147638508526822386", "to": "-8539761558200713310", "relation": "parentId"}, {"from": "4147638508526822386", "to": "2339257800227025559", "relation": "parentId"}, {"from": "4144293079476866784", "to": "-8554366541556919582", "relation": "parentId"}, {"from": "4144293079476866784", "to": "2160399242215679546", "relation": "parentId"}, {"from": "4144293079476866784", "to": "-6509981157393593334", "relation": "parentId"}, {"from": "4144293079476866784", "to": "4089777812670243708", "relation": "parentId"}, {"from": "4089777812670243708", "to": "1308388098926162502", "relation": "parentId"}, {"from": "4089777812670243708", "to": "2949554739902585927", "relation": "parentId"}, {"from": "4089777812670243708", "to": "3116073812452461853", "relation": "parentId"}, {"from": "4089777812670243708", "to": "-8379321662745430291", "relation": "parentId"}], "name": "test", "pathNodes": ["4147638508526822386", "4144293079476866784", "4089777812670243708", "3116073812452461853"], "focusedId": "3116073812452461853"}