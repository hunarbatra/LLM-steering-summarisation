{"nodes": [{"id": "-8723633167758947774", "text": "You are an intelligent summariser and a professional writer. You are given a piece of text and it's overview structure. You will use the text and overview structure to write a cohesive and detailed summary in 250 words step by step. You will cover all important aspects and key ideas presented in the text. Make sure that the generated text is not redundant, and it should be professionally summarised. Be sure to prioritise adding important details and information from the text in the summary.\n\n        It\u2019s conceivable that we will build \u201cprosaic\u201d AGI, which doesn\u2019t reveal any fundamentally new ideas about the nature of intelligence or turn up any \u201cunknown unknowns.\u201d I think we wouldn\u2019t know how to align such an AGI; moreover, in the process of building it, we wouldn\u2019t necessarily learn anything that would make the alignment problem more approachable. So I think that understanding this case is a natural priority for research on AI alignment.\n\nIn particular, I don\u2019t think it is reasonable to say \u201cwe\u2019ll know how to cross that bridge when we come to it,\u201d or \u201cit\u2019s impossible to do meaningful work without knowing more about what powerful AI will look like.\u201d If you think that prosaic AGI is plausible, then we may already know what the bridge will look like when we get to it: if we can\u2019t do meaningful work now, then we have a problem.\n\n1. Prosaic AGI\nIt now seems possible that we could build \u201cprosaic\u201d AGI, which can replicate human behavior but doesn\u2019t involve qualitatively new ideas about \u201chow intelligence works:\u201d\n\nIt\u2019s plausible that a large neural network can replicate \u201cfast\u201d human cognition, and that by coupling it to simple computational mechanisms\u200a\u2014\u200ashort and long-term memory, attention, etc.\u200a\u2014\u200awe could obtain a human-level computational architecture.\nIt\u2019s plausible that a variant of RL can train this architecture to actually implement human-level cognition. This would likely involve some combination of ingredients like model-based RL, imitation learning, or hierarchical RL. There are a whole bunch of ideas currently on the table and being explored; if you can\u2019t imagine any of these ideas working out, then I feel that\u2019s a failure of imagination (unless you see something I don\u2019t).\nWe will certainly learn something by developing prosaic AGI. The very fact that there were no qualitatively new ideas is itself surprising. And beyond that, we\u2019ll get a few more bits of information about which particular approach works, fill in a whole bunch of extra details about how to design and train powerful models, and actually get some experimental data.\n\nBut none of these developments seem to fundamentally change the alignment problem, and existing approaches to AI alignment are not bottlenecked on this kind of information. Actually having the AI in front of us may let us work several times more efficiently, but it\u2019s not going to move us from \u201cwe have no idea how to proceed\u201d to \u201cnow we get it.\u201d\n\n2. Our current state\n2a. The concern\nIf we build prosaic superhuman AGI, it seems most likely that it will be trained by reinforcement learning (extending other frameworks to superhuman performance would require new ideas). It\u2019s easy to imagine a prosaic RL system learning to play games with superhuman levels of competence and flexibility. But we don\u2019t have any shovel-ready approach to training an RL system to autonomously pursue our values.\n\nTo illustrate how this can go wrong, imagine using RL to implement a decentralized autonomous organization (DAO) which maximizes its profit. If we had very powerful RL systems, such a DAO might be able to outcompete human organizations at a wide range of tasks\u200a\u2014\u200aproducing and selling cheaper widgets, but also influencing government policy, extorting/manipulating other actors, and so on.\n\nThe shareholders of such a DAO may be able to capture the value it creates as long as they are able to retain effective control over its computing hardware / reward signal. Similarly, as long as such DAOs are weak enough to be effectively governed by existing laws and institutions, they are likely to benefit humanity even if they reinvest all of their profits.\n\nBut as AI improves, these DAOs would become much more powerful than their human owners or law enforcement. And we have no ready way to use a prosaic AGI to actually represent the shareholder\u2019s interests, or to govern a world dominated by superhuman DAOs. In general, we have no way to use RL to actually interpret and implement human wishes, rather than to optimize some concrete and easily-calculated reward signal.\n\nI feel pessimistic about human prospects in such a world.\n\n2b. Behaving cautiously\nWe could respond by not letting powerful RL systems act autonomously, or handicapping them enough that we can maintain effective control.\n\nThis leads us to a potentially precarious situation: everyone agrees to deploy handicapped systems over which they can maintain meaningful control. But any actor can gain an economic advantage by skimping on such an agreement, and some people would prefer a world dominated by RL agents to one dominated by humans. So there are incentives for defection; if RL systems are very powerful, then these incentives may be large, and even a small number of defectors may be able to rapidly overtake the honest majority which uses handicapped AI systems.\n\nThis makes AI a \u201cdestructive technology\u201d with similar characteristics to e.g. nuclear weapons, a situation I described in my last post. Over the long run I think we will need to reliably cope with this kind of situation, but I don\u2019t think we are there yet. I think we could probably handle this situation, but there would definitely be a significant risk of trouble.\n\nThe situation is especially risky if AI progress is surprisingly rapid, if the alignment problem proves to be surprisingly difficult, if the political situation is tense or dysfunctional, if other things are going wrong at the same time, if AI development is fragmented, if there is a large \u201chardware overhang,\u201d and so on.\n\nI think that there are relatively few plausible ways that humanity could permanently and irreversibly disfigure its legacy. So I am extremely unhappy with \u201ca significant risk of trouble.\u201d\n\n2c. The current state of AI alignment\nWe know many approaches to alignment, it\u2019s just that none of these are at the stage of something you could actually implement (\u201cshovel-ready\u201d)\u200a\u2014\u200ainstead they are at the stage of research projects with an unpredictable and potentially long timetable.\n\nFor concreteness, consider two intuitively appealing approaches to AI alignment:\n\nIRL: AI systems could infer human preferences from human behavior, and then try to satisfy those preferences.\nNatural language: AI systems could have an understanding of natural language, and then execute instructions described in natural language.\nNeither of these approaches is shovel ready, in the sense that we have no idea how to actually write code that implements either of them\u200a\u2014\u200ayou would need to have some good ideas before you even knew what experiments to run.\n\nWe might hope that this situation will change automatically as we build more sophisticated AI systems. But I don\u2019t think that\u2019s necessarily the case. \u201cProsaic AGI\u201d is at the point where we can actually write down some code and say \u201cmaybe this would do superhuman RL, if you ran it with enough computing power and you fiddled with the knobs a whole bunch.\u201d But these alignment proposals are nowhere near that point, and I don\u2019t see any \u201cknown unknowns\u201d that would let us quickly close the gap. (By construction, prosaic AGI doesn\u2019t involve unknown unknowns.)\n\nSo if we found ourselves with prosaic AGI tomorrow, we\u2019d be in the situation described in the last section, for as long as it took us to complete one of these research agendas (or to develop and then execute a new one). Like I said, I think this would probably be OK, but it opens up an unreasonably high chance of really bad outcomes.\n\n3. Priorities\nI think that prosaic AGI should probably be the largest focus of current research on alignment. In this section I\u2019ll argue for that claim.\n\n3a. Easy to start now\nProsaic AI alignment is especially interesting because the problem is nearly as tractable today as it would be if prosaic AGI were actually available.\n\nExisting alignment proposals have only weak dependencies on most of the details we would learn while building prosaic AGI (e.g. model architectures, optimization strategies, variance reduction tricks, auxiliary objectives\u2026). As a result, ignorance about those details isn\u2019t a huge problem for alignment work. We may eventually reach the point where those details are critically important, but we aren\u2019t there yet.\n\nFor now, finding any plausible approach to alignment, that works for any setting of unknown details, would be a big accomplishment. With such an approach in hand we could start to ask how sensitive it is to the unknown details, but it seems premature to be pessimistic before even taking that first step.\n\nNote that even in the extreme case where our approach to AI alignment would be completely different for different values of some unknown details, the speedup from knowing them in advance is at most 1/(probability of most likely possibility). The most plausibly critical details are large-scale architectural decisions, for which there is a much smaller space of possibilities.\n\n3b. Importance\nIf we do develop prosaic AGI without learning a lot more about AI alignment, then I think it would be bad news (see section 2). Addressing alignment earlier, or having a clear understanding of why it intractable, would make the situation a lot better.\n\nI think the main way that an understanding of alignment could fail to be valuable is if it turns out that alignment is very easy. But in that case, we should also be able quickly to solve it now (or at least have some candidate solution), and then we can move on to other things. So I don\u2019t think \u201calignment is very easy\u201d is a possibility that should keep us up at night.\n\nAlignment for prosaic AGI in particular will be less important if we don\u2019t actually develop prosaic AGI, but I think that this is a very big problem:\n\nFirst, I think there is a reasonable chance (>10%) that we will build prosaic AGI. At this point there don\u2019t seem to be convincing arguments against the possibility, and one of the lessons of the last 30 years is that learning algorithms and lots of computation/data can do surprisingly well compared to approaches that require understanding \u201chow to think.\u201d\n\nIndeed, I think that if you had forced someone in 1990 to write down a concrete way that an AGI might work, they could easily have put 10\u201320% of their mass on the same cluster of possibilities that I\u2019m currently calling \u201cprosaic AGI.\u201d And if you\u2019d ask them to guess what prosaic AGI would look like, I think that they could have given more like 20\u201340%.\n\nSecond, even if we don\u2019t develop prosaic AGI, I think it is very likely that there will be important similarities between alignment for prosaic AGI and alignment for whatever kind of AGI we actually build. For example, whatever AGI we actually build is likely to exploit many of the same techniques that a prosaic AGI would, and to the extent that those techniques pose challenges for alignment we will probably have to deal with them one way or another.\n\nI think that working with a concrete model that we have available now is one of the best ways to make progress on alignment, even in cases where we are sure that there will be at least one qualitative change in how we think about AI.\n\nThird, I think that research on alignment is significantly more important in cases where powerful AI is developed relatively soon. And in these cases, the probability of prosaic AGI seems to be much higher. If prosaic AGI is possible, then I think there is a significant chance of building broadly human level AGI over the next 10\u201320 years. I\u2019d guess that hours of work on alignment are perhaps 10x more important if AI is developed in the next 15 years than if it is developed later, just based on simple heuristics based on diminishing marginal returns.\n\n3c. Feasibility\nSome researchers (especially at MIRI) believe that aligning prosaic AGI is probably infeasible\u200a\u2014\u200athat the most likely approach to building an aligned AI is to understand intelligence in a much deeper way than we currently do, and that if we manage to build AGI before achieving such an understanding then we are in deep trouble.\n\nI think that this shouldn\u2019t make us much less enthusiastic about prosaic AI alignment:\n\nFirst, I don\u2019t think it\u2019s reasonable to have a confident position on this question. Claims of the form \u201cproblem X can\u2019t be solved\u201d are really hard to get right, because you are fighting against the universal quantifier of all possible ways that someone could solve this problem. (This is very similar to the difficulty of saying \u201csystem X can\u2019t be compromised.\u201d) To the extent that there is any argument that aligning prosaic AGI is infeasible, that argument is nowhere near the level of rigor which would be compelling.\n\nThis implies on the one hand that it would be unwise to assign a high probability to the infeasibility of this problem. It implies on the other hand that even if the problem is infeasible, then we might expect to develop a substantially more complete understanding of why exactly it is so difficult.\n\nSecond, if this problem is actually infeasible, that is an extremely important fact with direct consequences for what we ought to do. It implies we will be unable to quickly play \u201ccatch up\u201d on alignment after developing prosaic AGI, and so we would need to rely on coordination to prevent catastrophe. As a result:\n\nWe should start preparing for such coordination immediately.\nIt would be worthwhile for the AI community to substantially change its research direction in order to avoid catastrophe, even though this would involve large social costs.\nI think we don\u2019t yet have very strong evidence for the intractability of this problem.\n\nIf we could get very strong evidence, I expect it would have a significant effect on changing researchers\u2019 priorities and on the research community\u2019s attitude towards AI development. Realistically, it\u2019s probably also a precondition for getting AI researchers to make a serious move towards an alternative approach to AI development, or to start talking seriously about the kind of coordination that would be needed to cope with hard-to-align AI.\n\nConclusion\nI\u2019ve claimed that prosaic AGI is conceivable, that it is a very appealing target for research on AI alignment, and that this gives us more reason to be enthusiastic for the overall tractability of alignment. For now, these arguments motivate me to focus on prosaic AGI.\n        \n        \n        OVERVIEW STRUCTURE: I. Introduction II. Prosaic AGI     A. Replicating human behavior     B. Combination of ingredients     C. Learning something III. Our Current State     A. Concerns     B. Example of decentralized autonomous organization     C. Benefits of DAO IV. Feasibility     A. Difficulty of confidently claiming that prosaic AGI is infeasible     B. Implications of infeasibility     C. Lack of strong evidence for intractability V. Priorities     A. Prosaic AGI should be the largest focus of current research on alignment     B. Reasons for this claim VI. Conclusion     A. Prosaic AGI is conceivable     B. Appeal of prosaic AGI for research on AI alignment     C. Focus on prosaic AGI\n\n\n        SUMMARY:", "parentIds": [], "childrenIds": ["6068660368356125231", "7225864272972450225", "-8144987142533642311", "-6690294957692805294"], "lastVisited": "1676889344", "group": "path"}, {"id": "6068660368356125231", "text": " Prosaic AGI (Artificial General Intelligence) has the goal of replicating human behavior, and requires a combination of ingredients such as model architectures, optimization strategies and variance reduction tricks. There are definite concerns about its development, particularly with regards to the potential for it to be used to create a decentralized autonomous organization (DAO) that is more powerful than its human creators and the law enforcement.", "parentIds": ["-8723633167758947774"], "childrenIds": [], "lastVisited": "1676889344"}, {"id": "7225864272972450225", "text": " Prosaic AGI is a form of Artificial General Intelligence that involves the replication of human behaviour. It can be achieved by combining various ingredients such as deep learning, natural language processing, and reinforcement learning. Despite the appeal of this theory for AI alignment research, there are some concerns with its feasibility. Nonetheless, prosaic AGI should remain the largest focus of current research due to its potential to rapidly", "parentIds": ["-8723633167758947774"], "childrenIds": [], "lastVisited": "1676889344"}, {"id": "-8144987142533642311", "text": " Prosaic AGI is a type of artificial general intelligence (AGI) which can replicate human behavior and is seen as a promising avenue for research into AI alignment. It has the potential to be trained through reinforcement learning and used to create decentralized autonomous organizations that can outperform traditional human organizations. However, there is some concern that this may not be feasible due to the complexity of the task and the", "parentIds": ["-8723633167758947774"], "childrenIds": ["3909526017271949792", "-8019680032587526462", "3482290264204349835", "1452524374055815722"], "lastVisited": "1676889344", "group": "path"}, {"id": "-6690294957692805294", "text": " This paper investigates the potential and role of Artificial General Intelligence (AGI) in AI Alignment. It suggests that AGI could be replicated through a large neural network coupled with simple computational mechanisms such as short and long-term memory, attention and so on. While the current state of AI alignment is still in the research stage and not ready for implementation, it is argued that prosaic AGI is", "parentIds": ["-8723633167758947774"], "childrenIds": [], "lastVisited": "1676889344"}, {"id": "3909526017271949792", "text": " lack of strong evidence for its intractability. Despite this, research into prosaic AGI should remain a priority for AI alignment as it offers a sound and much-needed avenue for exploring the nature of intelligence. It is argued that prosaic AGI is conceivable and offers a range of potential benefits such as the potential for it to be achieved, the potential for it to greatly benefit society and its", "parentIds": ["-8144987142533642311"], "childrenIds": [], "lastVisited": "1676889344"}, {"id": "-8019680032587526462", "text": " lack of strong evidence for its intractability. Despite this, prosaic AGI should be the largest focus of current research on alignment due to its ability to create autonomous organizations and the appeal of creating machines that can accurately interpret and follow human instructions and preferences. If successful, this could lead to a world where machines are used to implement human wishes and maintain control over powerful AI systems. The author argues", "parentIds": ["-8144987142533642311"], "childrenIds": [], "lastVisited": "1676889344"}, {"id": "3482290264204349835", "text": " difficulty in confidently predicting the outcome. Nevertheless, prosaic AGI should still be the largest focus of current research on AI alignment. This is because it is conceivable and appealing for research on AI alignment and also has the potential to be deployed faster and more securely than other forms of AI. Moreover, understanding this type of AGI could allow us to understand the alignment problem better and be more prepared to tackle", "parentIds": ["-8144987142533642311"], "childrenIds": ["1913117650319917949", "4942052726735576365", "-6493432605968713108"], "lastVisited": "1676889344", "group": "path"}, {"id": "1452524374055815722", "text": " lack of strong evidence for its intractability. Nonetheless, the author believes that research into this area should be a priority as it could yield new insights and has the potential to be beneficial in terms of creating more efficient and advanced AI-powered organizations. Despite the difficulty of confidently claiming that the task is infeasible, the AI community should prepare for the need for coordination to prevent catastrophe if it is", "parentIds": ["-8144987142533642311"], "childrenIds": [], "lastVisited": "1676889344"}, {"id": "1913117650319917949", "text": " it when the time comes.", "parentIds": ["3482290264204349835"], "childrenIds": [], "lastVisited": "1676889344"}, {"id": "4942052726735576365", "text": " it in the future. Therefore, research on prosaic AGI should be prioritized in order to ensure that when the time comes to deploy it, we are ready to do so.", "parentIds": ["3482290264204349835"], "childrenIds": [], "lastVisited": "1676889344"}, {"id": "-6493432605968713108", "text": " it efficiently if it proves to be intractable. Ultimately, research into prosaic AGI should be our priority due to its potential advantages and the lack of strong evidence against its feasibility.", "parentIds": ["3482290264204349835"], "childrenIds": [], "lastVisited": "1676889344", "group": "path"}], "edges": [{"from": "-8723633167758947774", "to": "6068660368356125231", "relation": "parentId"}, {"from": "-8723633167758947774", "to": "7225864272972450225", "relation": "parentId"}, {"from": "-8723633167758947774", "to": "-8144987142533642311", "relation": "parentId"}, {"from": "-8723633167758947774", "to": "-6690294957692805294", "relation": "parentId"}, {"from": "-8144987142533642311", "to": "3909526017271949792", "relation": "parentId"}, {"from": "-8144987142533642311", "to": "-8019680032587526462", "relation": "parentId"}, {"from": "-8144987142533642311", "to": "3482290264204349835", "relation": "parentId"}, {"from": "-8144987142533642311", "to": "1452524374055815722", "relation": "parentId"}, {"from": "3482290264204349835", "to": "1913117650319917949", "relation": "parentId"}, {"from": "3482290264204349835", "to": "4942052726735576365", "relation": "parentId"}, {"from": "3482290264204349835", "to": "-6493432605968713108", "relation": "parentId"}], "name": "test", "pathNodes": ["-8723633167758947774", "-8144987142533642311", "3482290264204349835", "-6493432605968713108"], "focusedId": "-6493432605968713108"}