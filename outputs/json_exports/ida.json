{"nodes": [{"id": "-4136611614783374139", "text": "You are an intelligent summariser and a professional writer. You are given a piece of text and it's overview structure. You will use the text and overview structure to write a cohesive and detailed summary in 250 words step by step. You will cover all important aspects and key ideas presented in the text. Make sure that the generated text is not redundant, and it should be professionally summarised. Be sure to prioritise adding important details and information from the text in the summary.\n\n        Iterated Distillation and Amplification\nThis is a guest post summarizing Paul Christiano\u2019s proposed scheme for training machine learning systems that can be robustly aligned to complex and fuzzy values, which I call Iterated Distillation and Amplification (IDA) here. IDA is notably similar to AlphaGoZero and expert iteration.\n\nThe hope is that if we use IDA to train each learned component of an AI then the overall AI will remain aligned with the user\u2019s interests while achieving state of the art performance at runtime\u200a\u2014\u200aprovided that any non-learned components such as search or logic are also built to preserve alignment and maintain runtime performance. This document gives a high-level outline of IDA.\n\nMotivation: The alignment/capabilities tradeoff\nAssume that we want to train a learner A to perform some complex fuzzy task, e.g. \u201cBe a good personal assistant.\u201d Assume that A is capable of learning to perform the task at a superhuman level\u200a\u2014\u200athat is, if we could perfectly specify a \u201cpersonal assistant\u201d objective function and trained A to maximize it, then A would become a far better personal assistant than any human.\n\nThere is a spectrum of possibilities for how we might train A to do this task. On one end, there are techniques which allow the learner to discover powerful, novel policies that improve upon human capabilities:\n\nBroad reinforcement learning: As A takes actions in the world, we give it a relatively sparse reward signal based on how satisfied or dissatisfied we are with the eventual consequences. We then allow A to optimize for the expected sum of its future rewards\nBroad inverse reinforcement learning: A attempts to infer our deep long-term values from our actions, perhaps using a sophisticated model of human psychology and irrationality to select which of many possible extrapolations is correct.\nHowever, it is difficult to specify a broad objective that captures everything we care about, so in practice A will be optimizing for some proxy that is not completely aligned with our interests. Even if this proxy objective is \u201calmost\u201d right, its optimum could be disastrous according to our true values.\n\nOn the other end, there are techniques that try to narrowly emulate human judgments:\n\nImitation learning: We could train A to exactly mimic how an expertwould do the task, e.g. by training it to fool a discriminative model trying to tell apart A\u2019s actions from the human expert\u2019s actions.\nNarrow inverse reinforcement learning: We could train A to infer our near-term instrumental values from our actions, with the presumption that our actions are roughly optimal according to those values.\nNarrow reinforcement learning: As A takes actions in the world, we give it a dense reward signal based on how reasonable we judge its choices are (perhaps we directly reward state-action pairs themselves rather than outcomes in the world, as in TAMER). A optimizes for the expected sum of its future rewards.\nUsing these techniques, the risk of misalignment is reduced significantly (though not eliminated) by restricting agents to the range of known human behavior\u200a\u2014\u200abut this introduces severe limitations on capability. This tradeoff between allowing for novel capabilities and reducing misalignment risk applies across different learning schemes (with imitation learning generally being narrowest and lowest risk) as well as within a single scheme.\n\nThe motivating problem that IDA attempts to solve: if we are only able to align agents that narrowly replicate human behavior, how can we build an AGI that is both aligned and ultimately much more capable than the best humans?\n\nCore concept: Analogy to AlphaGoZero\nThe core idea of Paul\u2019s scheme is similar to AlphaGoZero (AGZ): We use a learned model many times as a subroutine in a more powerful decision-making process, and then re-train the model to imitate those better decisions.\n\nAGZ\u2019s policy network p is the learned model. At each iteration, AGZ selects moves by an expensive Monte Carlo Tree Search (MCTS) which uses policy pas its prior; p is then trained to directly predict the distribution of moves that MCTS ultimately settles on. In the next iteration, MCTS is run using the new more accurate p, and p is trained to predict the eventual outcome of that process, and so on. After enough iterations, a fixed point is reached\u200a\u2014\u200ap is unable to learn how running MCTS will change its current probabilities.\n\nMCTS is an amplification of p\u200a\u2014\u200ait uses p as a subroutine in a larger process that ultimately makes better moves than p alone could. In turn, p is a distillation of MCTS: it learns to directly guess the results of running MCTS, achieving comparable performance while short-cutting the expensive computation. The idea of IDA is to use the basic iterated distillation and amplification procedure in a much more general domain.\n\nThe IDA Scheme\nIDA involves repeatedly improving a learned model through an amplification and distillation process over multiple iterations.\n\nAmplification is interactive and human-directed in IDA\nIn AGZ, the amplification procedure is Monte Carlo Tree Search\u200a\u2014\u200ait\u2019s a simple and well-understood algorithm, and there\u2019s a clear mechanism for how it improves on the policy network\u2019s original choices (it traverses the game tree more deeply). But in IDA, amplification is not necessarily a fixed algorithm that can be written down once and repeatedly applied; it\u2019s an interactive process directed by human decisions.\n\nIn most domains, humans are capable of improving their native capabilities by delegating to assistants (e.g. because CEOs can delegate tasks to a large team, they can produce orders of magnitude more output per day than they could on their own). This means if our learning procedure can create an adequate helper for the human, the human can use the AI to amplify their ability\u200a\u2014\u200athis human/AI system may be capable of doing things that the human couldn\u2019t manage on their own.\n\nBelow I consider the example of using IDA to build a superhuman personal assistant. Let A[t] to refer to the state of the learned model after the end of iteration t; the initial agent A[0] is trained by a human overseer H.\n\nExample: Building a superhuman personal assistant\nH trains A[0] using a technique from the narrow end of the spectrum, such as imitation learning. Here we are imagining a much more powerful version of \u201cimitation learning\u201d than current systems are actually capable of\u200a\u2014\u200awe assume that A[0] can acquire nearly human-level capabilities through this process. That is, the trained A[0] model executes all the tasks of a personal assistant as H would (including comprehending English instructions, writing emails, putting together a meeting schedule, etc).\n\nEven though A[0] cannot discover any novel capabilities, it has two key advantages over H: it can run much faster, and many copies or versions of it can be run at once. We hope to leverage these advantages to construct a larger system\u200a\u2014\u200ainvolving H and many copies of A[0]\u200a\u2014\u200athat will substantially improve on H\u2019s capabilities while preserving alignment with H\u2019s values.\n\nH can use calls to A[0] (along with other tools such as external memory) to become a better personal assistant. For example, H could assign one copy of A[0] to figuring out the best time to schedule the client\u2019s recurring team meetings, another copy to figure out what to order the client for lunch, another copy to balance the client\u2019s personal budget, etc. H now has the ability to get very quick solutions to sub-problems that are roughly as good as the ones H would have come up with on their own over a longer time period, and can combine these results to make much better decisions than an unaided human.\n\nLet Amplify(H, A[0]) refer to the larger system of H + many copies of A[0] + aids. Compared to A[0] alone, the Amplify(H, A[0]) system has much higher time and resource costs but its eventual decisions are much better. Moreover, because in each of its individual decisions each copy of A[0] continues to act just as a human personal assistant would act, we can hope that Amplify(H, A[0]) preserves alignment.\n\nIn the next iteration of training, the Amplify(H, A[0]) system takes over the role of H as the overseer. A[1] is trained with narrow and safe techniques to quickly reproduce the results of Amplify(H, A[0]). Because we assumed Amplify(H, A[0]) was aligned, we can hope that A[1] is also aligned if it is trained using sufficiently narrow techniques which introduce no new behaviors. A[1] is then used in Amplify(H, A[1]), which serves as an overseer to train A[2], and so on.\n\nWhat properties must hold for IDA to work?\nThe IDA scheme is a template with \u201cslots\u201d for Amplify and Distill procedures that have not been fully specified yet\u200a\u2014\u200ain fact, they rely on capabilities we don\u2019t yet have. Because IDA itself is not fully specified, it\u2019s not clear what minimal set of properties are necessary for it to succeed.\n\nAchieving alignment and high capability\nThat said, here are some general properties which seem necessary\u200a\u2014\u200athough likely not sufficient\u200a\u2014\u200afor IDA agents to achieve robust alignment and high capability:\n\nThe Distill procedure robustly preserves alignment: Given an aligned agent H we can use narrow safe learning techniques to train a much faster agent A which behaves as H would have behaved, without introducing any misaligned optimization or losing important aspects of what H values.\nThe Amplify procedure robustly preserves alignment: Given an aligned agent A, it is possible to specify an amplification scheme which calls A multiple times as a subroutine in a way that reliably avoids introducing misaligned optimization.\nAt least some human experts are able to iteratively apply amplification to achieve arbitrarily high capabilities at the relevant task: a) there is some threshold of general capability such that if someone is above this threshold, they can eventually solve any problem that an arbitrarily intelligent system could solve, provided they can delegate tasks to similarly-intelligent assistants and are given arbitrary amounts of memory and time; b) at least some human experts are above this threshold of generality\u200a\u2014\u200agiven enough time and resources, they can figure out how to use AI assistants and tools to improve their capabilities arbitrarily far.\nThe non-profit Ought is working on gathering more evidence about assumptions 2 and 3.\n\nAchieving competitive performance and efficiency\nPaul aims for IDA agents to be competitive with traditional RL agents in time and resource costs at runtime\u200a\u2014\u200athis is a reasonable expectation because an IDA agent is ultimately just another learned model whose weights were tuned with an unusual training procedure.\n\nResource and time cost during training is a more open question; I haven\u2019t explored the assumptions that would have to hold for the IDA training process to be practically feasible or resource-competitive with other AI projects.\n        \n        \n        OVERVIEW STRUCTURE: I. Introduction A. Iterated Distillation and Amplification (IDA) B. Similarity to AlphaGoZero and Expert Iteration II. Motivation A. Alignment/Capabilities Tradeoff B. Broad Reinforcement Learning C. Broad Inverse Reinforcement Learning D. Imitation Learning E. Narrow Inverse Reinforcement Learning F. Narrow Reinforcement Learning III. Core Concept A. Analogy to AlphaGoZero B. Definition of Amplify(H, A[0]) IV. Iterative Training A. Amplify(H, A[0]) as overseer B. Training A[1] with narrow and safe techniques V. Properties Necessary for IDA to Work A. Robustly preserving alignment B. Achieving alignment and high capability C. Achieving competitive performance and efficiency VI. Example: Building a superhuman personal assistant A. Training A[0] B. Advantages of A[0] C. Leveraging A[0] to improve H\u2019s capabilities D. Examples of how H can use A[0]VII. Ought's Work on Assumptions A. Resource and time cost during training VIII. Conclusion\n\n\n        SUMMARY:", "parentIds": [], "childrenIds": ["603007718434663454", "6668549250138211340", "2230278324409380807", "1890257042689583111"], "lastVisited": "1676889824", "group": "path"}, {"id": "603007718434663454", "text": " Iterated Distillation and Amplification (IDA) is a proposed scheme developed by Paul Christiano to create high-capability agents that remain aligned with complex and fuzzy values. It is an iterative method of training a machine learning system and is based on the same principle as AlphaGoZero. In IDA, a human expert\u2019s decisions are used to design a larger system. This", "parentIds": ["-4136611614783374139"], "childrenIds": [], "lastVisited": "1676889824"}, {"id": "6668549250138211340", "text": " Paul Christiano's Iterated Distillation and Amplification (IDA) approach seeks to bridge the gap between alignment and capability when training machine learning systems. The approach is similar to AlphaGoZero and Expert Iteration, and is motivated by the trade-off between alignment and capability. IDA involves amplifying human overseers with an initial agent A[0] through a process of iterative", "parentIds": ["-4136611614783374139"], "childrenIds": ["258540314792967266", "-216642648387744139", "-3820895025084188362", "668254162744623556"], "lastVisited": "1676889824", "group": "path"}, {"id": "2230278324409380807", "text": " Iterated Distillation and Amplification (IDA) is a proposed scheme by Paul Christiano which is designed to address the alignment/capabilities tradeoff. It is inspired by AlphaGoZero and Expert Iteration, a Monte Carlo Tree Search (MCTS) and a policy network, allowing for robustly preserving alignment while achieving alignment and higher capability. IDA works by initially training a model (", "parentIds": ["-4136611614783374139"], "childrenIds": [], "lastVisited": "1676889824"}, {"id": "1890257042689583111", "text": " Iterated Distillation and Amplification (IDA) is a process developed by Paul Christiano to efficiently and robustly train AI agents. It is a combination of distillation and amplification, similar to AlphaGoZero, and uses an interactive overseer called Amplify(H, A[0]). The goal of IDA is to build AI agents that can be aligned with complex and fuzzy values while", "parentIds": ["-4136611614783374139"], "childrenIds": [], "lastVisited": "1676889824"}, {"id": "258540314792967266", "text": " training. The agent is trained with narrow and safe techniques to quickly reproduce the results of the previous iteration. To make sure alignment is robustly preserved and a high capability is achieved, the Amplify and Distill procedures must be applied and some human experts must be able to iteratively apply amplification. An example application of this approach is building superhuman personal assistants, where A[0] is trained to perform", "parentIds": ["6668549250138211340"], "childrenIds": [], "lastVisited": "1676889824"}, {"id": "-216642648387744139", "text": " interactive training and distillation. This process is analogous to the distillation and amplification approach used to train AlphaGoZero. To ensure that alignment is robustly preserved in the iterative training, certain properties are necessary for IDA to work. An example of using IDA is building a superhuman personal assistant, in which A[0] is trained using imitation learning and then used to improve the human", "parentIds": ["6668549250138211340"], "childrenIds": ["3370563533473908832", "7345890193134963754", "-404387762726996571", "-1863694017151364825"], "lastVisited": "1676889824", "group": "path"}, {"id": "-3820895025084188362", "text": " training which uses narrow and safe techniques to rapidly reproduce the results of Amplify(H, A[0]). Such an approach could help build superhuman personal assistants, as A[0] can quickly generate solutions to problems that an overseer would have come up with on their own over a longer time period. Furthermore, the properties necessary for IDA to work include robustly preserving alignment, achieving alignment and", "parentIds": ["6668549250138211340"], "childrenIds": [], "lastVisited": "1676889824"}, {"id": "668254162744623556", "text": " training. The iterative training requires a distill process to produce a faster agent A[n] with the same capabilities as the previous iteration, and an amplify process to create a more capable agent. For IDA to work, alignment must be robustly preserved in order to achieve high capability and competitive performance and efficiency. An example of this technique is building a superhuman personal assistant, where A[0", "parentIds": ["6668549250138211340"], "childrenIds": [], "lastVisited": "1676889824"}, {"id": "3370563533473908832", "text": " overseer's capabilities. Ought, a non-profit, is working on understanding the resource and time cost associated with IDA. In conclusion, IDA offers an approach to both preserve alignment and achieve state-of-the-art performance at runtime.", "parentIds": ["-216642648387744139"], "childrenIds": [], "lastVisited": "1676889824"}, {"id": "7345890193134963754", "text": " overseer\u2019s capabilities. Non-profit Ought are currently working on gathering evidence about these necessary assumptions, as well as reducing the resource and time costs associated with IDA. Ultimately, if these assumptions are confirmed, it could offer an efficient and effective way to achieve both alignment and high capability in machine learning systems.", "parentIds": ["-216642648387744139"], "childrenIds": [], "lastVisited": "1676889824"}, {"id": "-404387762726996571", "text": " overseer's capabilities. Currently, the non-profit Ought is researching the assumptions necessary for IDA to work, in particular, the resource and time cost of training. If successful, IDA will be able to achieve both alignment and high capability while performing at a state-of-the-art level.", "parentIds": ["-216642648387744139"], "childrenIds": [], "lastVisited": "1676889824"}, {"id": "-1863694017151364825", "text": " overseer's capabilities. An important factor in making IDA successful is Ought's work on assumptions, such as the resource and time cost associated with the training of A[0]. IDA offers a promising avenue for training machine learning systems that can preserve alignment and achieve state of the art performance, ultimately exceeding the capabilities of the human overseer.", "parentIds": ["-216642648387744139"], "childrenIds": [], "lastVisited": "1676889824", "group": "path"}], "edges": [{"from": "-4136611614783374139", "to": "603007718434663454", "relation": "parentId"}, {"from": "-4136611614783374139", "to": "6668549250138211340", "relation": "parentId"}, {"from": "-4136611614783374139", "to": "2230278324409380807", "relation": "parentId"}, {"from": "-4136611614783374139", "to": "1890257042689583111", "relation": "parentId"}, {"from": "6668549250138211340", "to": "258540314792967266", "relation": "parentId"}, {"from": "6668549250138211340", "to": "-216642648387744139", "relation": "parentId"}, {"from": "6668549250138211340", "to": "-3820895025084188362", "relation": "parentId"}, {"from": "6668549250138211340", "to": "668254162744623556", "relation": "parentId"}, {"from": "-216642648387744139", "to": "3370563533473908832", "relation": "parentId"}, {"from": "-216642648387744139", "to": "7345890193134963754", "relation": "parentId"}, {"from": "-216642648387744139", "to": "-404387762726996571", "relation": "parentId"}, {"from": "-216642648387744139", "to": "-1863694017151364825", "relation": "parentId"}], "name": "test", "pathNodes": ["-4136611614783374139", "6668549250138211340", "-216642648387744139", "-1863694017151364825"], "focusedId": "-1863694017151364825"}